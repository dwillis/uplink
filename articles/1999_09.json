{
    "articles": [
        {
            "full_text": "Commit a felony in Minnesota and, chances are, you won't go to prison. Commit a crime while on probation and, chances are, you won't go to prison. We had heard it for years: that repeat offenders and even convicted offenders on probation can and often do re-offend with few consequences. In 1999, we decided to put it to the test. I applied for a newsroom fellowship that would allow me to temporarily jump from our public safety team to the projects team to work with Dan Browning, computer-assisted reporting editor. The move freed me from the daily grind of the police and courts beats to dig into the anecdotes and see if there were data to support them. Capturing crime data The first stop was Minnesota's Data Practices Act, the complex state statute that details which information collected and kept by public agencies is public - and which is not. We reviewed sections on arrest data, jail data and court services data regarding offenders on probation. The next step was to fire off letters to several metropolitan area counties, requesting the data we understood to be public. We wanted to first identify the people on probation in the Twin Cities area over the previous five years, then compare those data to jail bookings and court data to see if offenders on probation were being arrested and, if so, what happened afterwards. Getting the data took about three months. The state's two largest counties - Hennepin and Ramsey - refused to give us dates of birth of probationers, insisting the data were private under state law. The Hennepin County Sheriff's Office refused to give us dates of birth for offenders in the jail, again arguing that the data were private. Birth dates were critical information. The state has no standardized, unique identifier to track criminal suspects among counties or as they pass through the justice system. The tracking devices follow cases, not individuals. But by converting birth dates to strings and combining them with names to create key fields, we believed we could link several different criminal justice databases. Thus, birth dates were critical to ensuring that we were finding offenders who crossed county lines and to following individuals from arrest to court to prison and back into the community. The Star Tribune sued Hennepin and Ramsey counties for the data, but the lawsuit would take months. We decided to proceed the hard way, but were forced to change focus from probationers to repeat offenders. We wanted to examine who was booked most often, and what was happening to their cases after arrest. We had dates of birth for jail inmates in every Minnesota county but Hennepin. We figured we could match the Hennepin County data to other criminal records to capture dates of birth. We obtained millions of criminal justice records from the Minnesota Sentencing Guidelines Commission (results of every sentencing statewide, 1993-1997), the Minnesota Supreme Court (felony and gross misdemeanor court dockets statewide, 1993-1997), Department of Corrections data on adult offenders (1994-1998) and work release inmates (1994-1998), and Bureau of Criminal Apprehension conviction data (felonies and gross misdemeanors, 1984-1999). Recognizing repeaters We became the only organization in Minnesota to attempt to link these various databases to find comprehensive criminal histories of repeat offenders. Neither the state nor the counties had constructed anything to answer the questions we would be asking. By matching district court case numbers from the Hennepin County jail database to Hennepin County cases in the state Supreme Court database, we were able to fill in thousands of dates of birth from the court data. Doing individual court record searches on others allowed us to fill in more dates of birth for the most frequent offenders. And finally, using FoxPro, we created unique key fields using three different combinations of name (first-middle-last; first-middle initial-last; last) and date of birth for each table in our criminal justice archive. These matches enabled us to extract birth dates for thousands of other Hennepin County offenders. In the end, we filled in dates of birth for one-third of all people booked in Hennepin County over the five-year period we studied and, more importantly, the top 1,300 most frequent offenders. Those offenders accounted for the bulk of the bookings. One of our key findings was that the top 3.5 percent of the offenders accounted for 22 percent of the bookings in Hennepin County. We became the only organization in Minnesota to attempt to link these various databases to find comprehensive criminal histories of repeat offenders. Neither the state nor the counties had constructed anything to answer the questions we would be asking. To define \"chronic offender,\" we settled on a standard of more than 11 bookings in the five-year study period. Under that definition, we found at least 8,700 offenders getting booked over and over and over again. And we also found that the impact of chronic offenders was felt not only in the inner cities, but also in the suburbs and rural areas. The data backed up our hypothesis. But our work was just beginning. As with all good CAR projects, the data simply gave us a starting place. After about six weeks of data cleaning and writing string functions to query the various tables against each other, we flagged the top offenders in the seven-county metropolitan area of the Twin Cities. We then checked them against court and sentencing databases and began the task of finding the offenders and the victims to tell their stories. Meanwhile, my colleague Paul Gustafson worked on a separate but related project examining the work-release program run by the Minnesota Department of Corrections. Gustafson analyzed five years of data and found that the program was releasing more hardened criminals into the program, some of whom served as little as one-third of their sentence behind bars. We decided to publish this project together with the repeat offenders project because they are two ends of the same spectrum: a cheap, lenient corrections philosophy that has saved money but set criminals essentially free in the community. Many of these criminals escaped from supervision and some committed new crimes. Readers respond We published our series on July 18, 19 and 20. In the end, it was the very human stories of victims and repeat offenders that gripped many readers. But we also finally gave them a sense of the scope of the problem. Local television, local talk radio and National Public Radio picked up parts of our project. We've heard from dozens of readers, including some legislators and other elected officials who are calling for ways to monitor repeat offenders. And after publishing our series, Ramsey and Hennepin counties agreed to provide the dates of birth for probationers (and in Hennepin's case, for those booked into jail) for the five-year study period, thereby settling our lawsuit. James Walsh can be reached by e-mail jwalsh@startribune.com",
            "headline": "Catch and release",
            "author_name": "James Walsh",
            "author_title": "null",
            "month": "September",
            "year": 1999
        },
        {
            "full_text": "In the small towns and one-school communities across Georgia, life has historically revolved around the fate of the high school football team. I was already kicking around the idea of a season-long package that would reveal just how important the game is to these communities - the role of parent-run touchdown clubs; the level of local and state tax monies spent on football, particularly for coaching salaries; and the status enjoyed by high school football coaches. Then, the letter came across my desk. It was an invitation from the pastor of a congregation in Macon, writing in his capacity as secretary/treasurer of the Westside High School football booster club. The club, he explained, wanted to show off its new football complex. And if The Atlanta Journal-Constitution newspapers were so inclined, the boosters would gladly take a reporter up in a helicopter for a birds-eye view of the complex. The thought of parents raising almost $2 million to build a football complex - not on school property but rather on land owned by its football booster club - was enough to convince the newspaper's editors that this was a story worth pursuing. The idea behind \"Pay Dirt! The Big Business of High School Football\" was to explore what separates the winners from the losers, the box-office successes from the failures. To do so, I teamed up with David Milliron, the newspaper's director of computer-assisted reporting. We built a comprehensive database of the nearly 2,400 high school football coaches and assistant coaches in the state. Forming a game plan Since teacher salary and personnel information is kept at the local school district level, the newspaper compiled an electronic database from information obtained from each of the state's 180 school districts - including payroll records, teaching certificates and teaching assignments. The AJC also obtained the state's annual school report card data, (in electronic form), which contained such information as school enrollments and average district salary data for teachers, administrators and support personnel. The database was built in Microsoft Excel and the analysis performed in Microsoft Access. The data were collected using the state's Open Records Act, which mandates the release of such information. Once each school district responded, the data were entered into the computer, and a \"proof sheet\" was then printed out for each of the state's 180 school districts. The \"proof sheets\" were then mailed back to the school district for verification of the information. The AJC sent out a third round of mailings and made numerous telephone calls to obtain 100 percent compliance. The last survey was faxed to the newspaper by a secretary just prior to leaving her office to attend her mother's funeral on the day before publication. While Milliron handled the survey and data collection, I spent my time interviewing coaches, administrators and booster club members. Many schools and administrators thought we were prying and refused to cooperate, but some expressed pride in their accomplishments. I eventually narrowed my reporting down to three schools to illustrate how the big business of high school football has evolved in Georgia. What the newspaper found was a startlingly well-financed support system for the game, one rivaling the model found on big-time college football campuses. Statistics suggest that interest in high school football is waning nationwide, but that certainly isn't true in Georgia. The established programs in southern Georgia continue to drum up huge financial and fan support. But increasingly, the Friday night fever has spread to the wealthier Atlanta suburbs, evidenced by the fancy stadiums - some with their own lounge boxes, spiffy weight rooms and well-appointed booster club headquarters. The final score Georgia may still struggle to shore up a comparatively weak public school academic record, but schools and taxpayers spare little expense to field competitive football teams. Our package questioned whether education is being compromised for playing games. Consider: $79.6 million in state tax dollars went to pay coaches' teaching contracts last year, although nearly a third have minimal or no teaching workload. Georgia taxpayers contribute through local school taxes another $9.6 million annually in supplemental pay for football coaches. Head football coaches make an average salary of nearly 55 percent more than the average for the state's teachers - $55,686 compared with $36,042. The earnings of 10 individual coaches each exceed the $75,724 annual pay for the state's lieutenant governor. Two coaches were found to earn higher salaries than their school principals, putting the schools at odds with certification standards of the Southern Association of Colleges and Schools. (It's likely both principals will be getting raises.) Seven percent of all state coaches are either ineligible to teach or hold conditional or probationary teaching certificates. Many parent-run booster clubs provide cash bonuses to coaches. The more successful coaches are rewarded with vacation trips and the keys to leased vehicles. These football booster clubs routinely operate as non-profits outside the jurisdictions of the schools, and at least one dub was found to have run a foul of IRS guidelines. The investigative package brought a huge readership response, both from the daily publication and the newspaper's Internet site. Some 300 e-mail messages, phone calls and letters showed up in the newsroom. And the newspaper's Internet site / which put up a searchable database of all the football coaches salaries and data - received an average of 1,300 hits an hour during the week of publication. Still today the database gets an average of 800 hits a day, with nearly 1 million searches conducted since the package was published last December. The package is accessible online at www.ajc.com/reportspagedirt Two coaches were foundto earn higher salaries than their school principals As a result of the AJC inquiry, the touchdown club at a suburban Atlanta high school which has an annual budget in excess of a quarter million dollars and which has raised $2.5 million for its new football complex retained an attorney and accountant to get the organization in compliance with state and federal tax laws. The Secretary of State previously dissolved the non-profit organization for failing to file annual reports and the AJC found the touchdown club hadn't filed tax returns with the IRS for at least three years. The package also alerted the IRS to the many taxable perks afforded some coaches from cash bonuses to vehicles to vacation trips. Stumbling blocks Gathering salary and teaching assignment information from all 312 public high schools proved to be a daily battle. Initially, some districts refused to comply or said it would cost hundreds of dollars for them to produce the information. We gathered information right up to the last hour. A lot of data were returned incomplete- either they didn't list coaches' names, omitted their teaching responsibilities or failed to differentiate football earnings from base teaching salaries. We constantly faxed and mailed requests to the districts. Finally, we shipped our data to the individual school districts and had them sign off on the accuracy of what we planned to publish regarding their coaches. The booster or touchdown clubs posed a different problem because they are set up outside the school as private non-profits. We got a grasp on budgets and annual income from reviewing IRS Form 990 tax records. But in almost all cases, it came down to piecing together information from a variety of current and past club officers, coaches and athletic directors. In the end, trying to get reactions from educators and public officials proved a surprisingly daunting chore. Outgoing Gov. Zell Miller, who championed education and promoted the state lottery to fund the state Hope Scholarship Program, said it was too hot a subject to take on in his last days. Lt. Gov. Pierre Howard didn't respond to interview requests. And Linda Schrenko, head of the state Education Department, turned down repeated interview requests on the subject, including a personal visit to her office two days before the articles were published. Tips for new players Next time, we'd allow more lead-time to gather and obtain the relevant documents and records requests. It requires a great deal of diligent follow-up to get a 100 percent response when requesting information from 312 schools. It's also important to clearly define early on what you're seeking in the open records request. We'd probably have been better served to start with a small school sample to get a better idea of what kind of information we could expect to get back. Then we could perfect our request for data relatively easily. Mike Fish can be reached by e-mail at mfish@ajc.com",
            "headline": "School sports as big business",
            "author_name": "Mike Fish",
            "author_title": "null",
            "month": "September",
            "year": 1999
        },
        {
            "full_text": "Missouri lawmakers are an avaricious lot. Many states prohibit or severely limit gifts from lobbyists on the premise that no one gives you something for nothing. But Missouri is one of 19 states that put no restrictions on lobbyists' gifts. As a result, a spending spree surrounds every legislative session, feeding a culture that sees no conflict in conducting the public's business while on a corporate tab. The one nod the law makes to public interest is a requirement that lobbyists report their spending on individual lawmakers to the state Ethics Commission. Though filled with loopholes, the monthly reports offer a glimpse at the gifts used to appease lawmakers: Cases of wine and beer. Tickets to see pop singer Alanis Morissette and tenor Luciano Pavarotti. Season passes to the St. Louis Cardinals. One lawmaker took $780 in tickets to the St. Louis Blues. Build a database To get a handle on the extent of the spending, the Star built a database of all 3,438 gifts given to lawmakers during this year's 4 1/2-month session. The conclusion: A lobbying technique that began as a bit of clubby salesmanship has evolved into a legislature on the dole. The Starput the master database on its Web site so readers could call up the gifts each lawmaker received. The analysis determined that lobbyists gave individual lawmakers gifts worth $139,219. That includes only one-on-one gifts and does not include the money spent on the constant stream of receptions, parties and meals for groups of legislators. The top recipient personally received gifts worth $5,843, or nearly $1,300 a month. Only once during the entire session did he go a week without taking something. The analysis also showed that: House Democrats took 63 percent of the gifts; 30 percent of the gifts involved tickets, lodging or transportation; One in four lawmakers took more than $1,000 in gifts; Three lawmakers received more than $1,000 in freebies from a single lobbyist. The analysis was done using Excel 5.0 and FoxPro 2.6. The first problem was the volume of the gifts, which are listed only on handwritten reports. The newspaper hired two people to enter the early data, but interns and reporters had to finish it. Start with Excel The resulting spreadsheet listed the lawmaker, his or her title and party, the lobbyist, the date of the transaction and a description of the gift and its value. The biggest weakness in the data was the way lobbyists described their gifts. While some lobbyists listed \"beverages,\" other lobbyists listed the same gift as \"food and/or beverages.\" Others combined freebies, describing them as \"tickets/parking/meal.\" Move to FoxPro I used Excel's Filter command - the Extract command in earlier versions to divide the gifts into different categories, and pasted them into other spreadsheets in the same workbook. I eventually combined them into five general categories: food & drink; tickets; lodging, transportation and entertainment; gifts; and all other. I then went through each category to eliminate redundancies. Gifts described as two things were picked up twice, others three times. To eliminate those, I sorted the spreadsheets by lawmaker, date and amount so the same records would be in adjacent rows. I then used a logical function to find rows that exactly matched the row next to it. After eliminating the redundant records, I saved each of those spreadsheets and the master list in Excel 4.0. Then I imported them into FoxPro databases. That allowed me to total all the gifts by lawmaker, lobbyist, party, chamber or date. The biggest expenditures were on the first day of the session; many of the other big days came in the last three weeks of the session. One thorny problem was revisions in the records. State law allows lawmakers to have the record of the gift expunged if they dispute the expenditure or repay the lobbyist. The paper began building the database in March. By May, a few dozen gifts were no longer listed in the records. We decided to base the articles on the final versions. One representative, for example, had taken his family on a lobbyist-paid, four-day vacation to Branson, Mo., in March. After he was interviewed about the trip in June, he repaid the lobbyist, so the trip was not listed on the final reports. The entire project took a little more than three weeks - a week of data entry, three days to crunch the numbers and about eight days to report and write. The Star put the master database on its Web site so readers could call up the gifts each lawmaker received. Three lawmakers received more than $1,000 in freebies from a single lobbyist. All the lobbyists and the recipients of large amounts insisted that the gifts had no effect on legislation. Then why, the biggest recipient was asked, do lobbyists spend all that money? \"What kind of stupid question is that?\" he replied. \"I'm not going to answer that. How would I know?\" Kit Wagar can be reached by e-mail at kwagar@kcstar.com",
            "headline": "'Out to the ball game'",
            "author_name": "Kit Wagar",
            "author_title": "null",
            "month": "September",
            "year": 1999
        },
        {
            "full_text": "Most political action committees are lit- erally - big business. In our case, the best way to make use of PAC data is \"joining\" - that is, using a computer to match up material in separate data files. Staff Writer Noel Oman alerted us that for the first time, the Arkansas secretary of state's office could provide electronic data for PAC contributions. The General Assembly was in session - perfect timing for a story. The result was a fairly quick-hit story, written by Oman and accompanied by graphics, for Arkansas Democrat-Gazette readers with more details than ever reported about PAC contributions to Arkansas lawmakers. Free data But we needed more information. We obtained data kept by legislative staff, including information on lawmakers and their committee assignments. To their credit, both arms of state government - the secretary of state's office with PAC data and the Arkansas Bureau of Legislative Research with lawmaker information - had no problem handing over the data tables on diskettes. Cost: nothing. Getting the files was just the beginning. We had to put things together, wanting to accomplish several things. First, we wanted to identify the biggest PAC contributors, so we asked Visual FoxPro to show a total of contributions from each PAC. Instantly, we saw the biggest players. We also examined the top recipients. Thanks to Oman's experience and an electronic file from the secretary of state's office, we could identify PACs related to one another, representing the same companies or business interests. Now the fun began: trying to match up lawmakers who got PAC money and their committee assignments. This year's legislative session had issues including electric utility deregulation, telemarketing restrictions and highway spending and tax plans. So we wanted to look at both contributors and recipients on key committees. Matching PACS to committees Here's a list of things we did: First, in both PAC data and legislative membership data, we broke the files in two, creating separate files for each legislative chamber. Why? Because we planned later to match PAC contributions with committee membership tables, using two factors to make the match: the lawmaker's last name and the district number the lawmaker represents. In just a few cases, Senate and House members might have the same district number and the same last name. Say, Rep. Brown and Sen. Brown with identical district numbers, even though they serve in different chambers. We obtained data kept by legislative staff, including information on lawmakers and their committee assignments .both arms of state government ... had no problem handing over the data tables on diskettes. That gave us a list showing how much PAC money legislative committee members received. With those files, we could easily pull up, examine and analyze contributions from various PACs to specific committee members. One caveat for this file that we had to observe: the contributions were repeated, because lawmakers had more than one committee assignment. But since for this file we wanted to break contributions down by committee, it's exactly what we needed. We used the previous files for looking at lawmaker PAC contributions overall. The results We found the usual \"suspects\" and no hidden skeletons. However, Oman's reporting, accompanied by graphics, did inform and educate readers. Here's a clip: Bob Balhorn sat in the audience last Wednesday when the House Insurance and Commerce Committee took up a bill to enable Arkansans to ban telemarketing calls to their homes. His was not just another face in the crowd. Balhorn, 63, of Little Rock is executive vice president of the Arkansas Realtors Association, the state affiliate of the National Association of Realtors. It is a post he has held since 1985, a tenure that covers eight regular sessions of the Arkansas General Assembly. Before that, the native Texan held a similar post in South Dakota, where he also sold real estate. His association, 6,000 members strong, does not immediately come to mind when the most powerful special interests at the state Capitol are assessed. But in the 1998 elections, no other single trade organization or corporation gave more to candidates for state office in Arkansas, according to an Arkansas Democrat-Gazette review of political action committee reports at the Secretary of State's office. The two committees registered to the association gave $78,800 to candidates for state offices in 1998. Of the total, $72,800 went to legislative candidates, the given to legislative office-seekers by any state-registered political action committee. The association's generosity in 1998 was no anomaly. In the 1996 election, the Realtors' association gave $63,350 to sitting lawmakers, second only to the nursing home industry, which reported giving $84,450. The nursing home industry's state umbrella organization, the Arkansas Health Care Association, reported giving $57,000 in 1998. And for a plus, we placed the PAC contribution file on our intranet, The Source, so all our editors and reporters could easily look up contributions for state candidates from the 1998 campaign season. Jeff Porter can be reached by e-mail at jporter@ardemgaz.com",
            "headline": "PAC Power",
            "author_name": "Jeff Porter",
            "author_title": "null",
            "month": "September",
            "year": 1999
        },
        {
            "full_text": "There is but one argument in computer- assisted reporting that rivals that of PC versus Mac. It gets blood boiling and inspires people to write opinionated tomes on the NICAR-L listserv. It's the debate over the merits of SAS versus SPSS. Users of the statistical programs each trumpet the advantages of their pet program (\"SPSS is easier!\" \"SAS is more powerful!\") but occasionally one can hear them muttering over the disadvantages inherent in each: \"SPSS can't do what I want it to.\" \"SAS is just too expensive.\" So which is the better program? The experts say it really depends on your needs in your newsroom. The pluses According to most, SPSS is usually best for beginners in CAR. One important factor is its relatively low cost - about $1000 for commercial use, half that for academic use and even less for individual students (both graduate and undergraduate versions are available). It also requires less complex commands and is quick to learn. Philip Meyer, Knight Chair and professor of journalism at University of North Carolina-Chapel Hill, is confident in the program's usefulness for CAR work: \"SPSS will handle every statistical problem that I can think of that a journalist might encounter. Including some weird-science stuff like logistic regression and factor analysis.\" Finally, SPSS is easy to understand Meyer calls this factor \"transparency.\" \"Transparency means being able to see right through the complicated computer operations as though they weren't even there. All you see is your input and the output. Another expression for transparency is 'ease of use, he says. \"Every computer system designer has to choose between transparency and flexibility. SPSS leans toward transparency, SAS toward flexibility.\" Which brings us to SAS's main advantage: it can do more than SPSS, a benefit that the SAS users tout, though their voices may be drowned out by those of the more numerous SPSS users. SAS can bring in a number of different data types, and it creates detailed logs each time programs are run, making it easier to dig out any bad data. Because of its ability to handle very large data sets, oftentimes SAS is used by the professional agencies that supply reporters with data. Steve Doig, professor at the Cronkite School of Journalism at Arizona State University, describes SAS as \"extremely powerful and full-featured. \"You just won't outgrow this, Doig says. The minuses Despite their preferences for one or the other, users of both programs are honest about the disadvantages inherent in each, which are sometimes related to the advantages. SAS's power, for example, comes at a cost - financial as well as the time required to learn the more complex procedures. \"You have to write programs,\" Doig says. \"There are pulldown menus, but they don't do many of the programming functions that the SPSS interface does.\" Also, the monetary cost can be prohibitive, especially for smaller newspapers or television stations. A license must be purchased to run SAS, rather than just a neat, cellophane-wrapped package of software. A first-year license fee for a basic SAS program will run you $2,200-2,300. The yearly renewal cost, though, is much cheaper - about half the initial amount. Any stats modules licensed on top of that basic program cost an extra $1,100-1,200. But the base program, which Doig says can handle chi-squares, frequency tables, etc., should be sufficient for most work. Expense and complexity both limit the number of CAR reporters who become SAS users, which might be a problem if you want to ask questions about your work. But Doig says there is help available: \"We few SAS zealots are so tickled when someone does want to use it that we're generally happy to chime in.\" SPSS is cheaper, but has its own drawbacks, It can't handle large data loads as SAS can, nor can it do complex file-management tasks as well. A few NICAR-L complaints about SPSS have included its limited ability to handle text fields, a lack of some key diagnostics for basic statistical procedures and less efficient data cleaning than SAS. Though he recommends SPSS for beginners in CAR, Meyer acknowledges its shortcomings in terms of flexibility. \"Bottom line: you can do especially file-management tasks, with SAS,\" he says. \"For those with the guts to stick out the SAS learning curve, it will last them for- ever. Those fainter of heart should start with SPSS. They can always get an outside contractor to do the slice-and-dice chores.\" What's right for you? In the end, it all boils down to a few basic questions: a) How much intensive statistical work do you need to do? b) How much statistical experience do you have? How much can you or your organization afford? If you answer \"A lot\" to all three, SAS may be your best bet. If it's \"Not much\" across the board, SPSS is probably more suitable. Any combination of the two responses means you need to weigh the pros and cons of each and maybe post a question or two to NICAR-L in hopes that a fervent SAS or SPSS user can sway you to one side or the other. Jessica Larson can be reached by e-mail at jessica@nicar.org",
            "headline": "SAS vs. SPSS",
            "author_name": "Jessica Larson",
            "author_title": "NICAR Staff",
            "month": "September",
            "year": 1999
        },
        {
            "full_text": "This is the second of two parts on how to do a regression analysis. This issue's column deals with regression outliers, the t-test and making transformations. In the last issue, the main advice was not to succumb to the \"black box\" syndrome - being awed by the ease of running regression analyses through computer programs with no appreciation for what is going on behind the curtain. The math behind the regression process was explained in detail. The regression model seeks to fit a line through - or close to - all of your data points. In a plot of the data, the x-axis holds the independent variable and the y-axis holds the dependent variable. That line is one that you might recall from high school algebra: y = ax + b + random error. The \"y\" is your dependent variable (such as test scores). The \"x\" is your independent variable (such as poverty rates). The \"a\" is the slope of the line and the \"b\" is the constant (the intercept of the vertical axis). This is the equation of the regression line. You may have seen the equation stated in different ways. Some statistics texts prefer the notation of y = bx + a. Ory=mx+ b. Or, even fancier, the equation y + Blx E Select whichever you're most comfortable with. The goal in regression is to show how much the dependent variable is related to the independent variable. This quest to find the truly predictive model is one undertaken by social scientists, physicists, epidemiologists, quality-control managers and others in nearly every field. If you're interested in practicing regression, there are plenty of free, downloadable datasets available on the Internet. Two great sites are the Journal of Statistics Education Data Archive at www.amstat.org/ publications/jselarchive.htm and the Data and Story Library at http://lib.stat.cmu.edu/DASL/, where you'll even find datasets covering acorns and oak trees and passenger car mileage. In most cases, the size of the dataset is small, making regression with MS Excel and other spreadsheet programs relatively easy. Here are some points to remember in the rush to regress. Dealing with outliers An outlier is a data point (observation, record, etc.) that represents an extreme value when compared with other values in the data. Outliers can be gremlins in all data analysis, whether you're calculating the mean or running a complex regression. If you're going to be \"hanging your hat\" on your regression results understand the t and the tests. If you don't pay attention to these tests, your whole hypothesis may be rejected without your knowledge. The simple way of thinking about an outlier is in terms of its effect on the average. Consider a study of speed in fatal automobile accidents. The estimated speeds of the cars involved in such accidents on one stretch of road in one year are 65, 55, 95, 60 and 65. The average speed is 68. Yet, all but one speed is less than 68. The 95-miles-per-hour speed skews the data. The average without the extreme value is 61 - certainly more representative of the group. Extreme values or outliers also can affect the outcome of your regression analysis. Many of us have seen the outliers in a scatterplot of test scores, with the percent of students eligible for free lunch as the independent variable. We see relatively poor schools with unusually high grades (the \"overachievers\") and relatively wealthy schools with unusually low grades (the \"underachievers\"). In these cases, the news may be in the outliers. What secrets might you find in the poor schools that do well on tests? However, a common practice in regression analysis is to throw out the outliers to try to achieve a higher coefficient of determination (r2). With the outliers included, maybe your r2 is 0.53, meaning that 53 percent of the variability in test scores is explained or predicted by the variability in income. By eliminating the outliers, your r2 may increase by several points - making you feel even more confident about a relationship. How far off are some data points? In SPSS, SAS and EXCEL, for example, you can save and study the residuals, which are the differences between the actual value of each observation and the predicted value from your regression model. You also can calculate the residuals by hand by substituting each \"x\" into the regression equation to solve for \"y\" (y = ax + b). Then find the difference between the actual \"y\" (for example, the test score) and the predicted \"y\" from the regression equation. (You can visualize residuals in a scatterplot of your data by drawing vertical lines from the datapoints to the regression line. The lengths of the vertical lines represent the residuals.) Watching the t-test The t test and the F test are part of the regression output in statistical programs and Excel. Yet, many people may pay no attention to these tests in their zeal to achieve the highest-possible r2. Pay attention to these tests, which report both the t and the F values, as well as their significance levels. These tests will indicate if there is a serious problem with the regression model. The ideal situation is to see \".000\" or something close to zero in the significance box. If you see much more than zero, this is cause for concern. If you're going to be \"hanging your hat\" on your regression results, spend some time with statistics texts to understand the t and the F tests. They are not difficult to understand, but they are involved. If you don't pay attention to these tests, your whole hypothesis may be rejected without your knowledge. Making transformations If you plot the data and find there are curves or bends in the fitted line, you may want to express the data differently, or \"transform\" it. I remember discussing this process with a colleague at an IRE and NICAR seminar and he gasped at the thought of changing the data. However, transformations are uniform and change all of the data by the same amount-much like multiplying both sides of an algebra equation by 2 to get rid of the 1/2 on one side of the equation. Depending on the severity and direction of the curve in the fitted line, you may want to transform either the independent (\"x\") or the dependent (\"y\") variable. Methods include substituting the square or square root of \"x\" or \"y\" or taking the natural logarithm of one of the variables. The goal is a better fit - a higher r2 and a straighter regression line. Clearly, there is a lot to know about regression analysis. It takes more than simply running a regression using SPSS or Excel and relying on each program's online Help facility to explain what you are doing. Pick up a statistics text or two - particularly those used in graduate school. Many college statistics departments have put up Web sites that explain the basics. Some actual statistics classes are online and available to the public. A good place to start is in Yahoo at http://dir.yahoo.com/ SciencelMathematics/Statisticsl But, most important of all, know that there are a lot of gears whirring in the black box and that it's worthwhile understanding a bit about how they mesh (or fail to mesh). Neill Borowski can be reached by e- mail at nborowski@phillynews.com",
            "headline": "Returning to regress",
            "author_name": "Neill A. Borowski",
            "author_title": "null",
            "month": "September",
            "year": 1999
        },
        {
            "full_text": "When the president of the United Teachers of Lowell denounced on local radio my investigation into teachers' absenteeism, saying, \"You can interpret the numbers to say whatever you want,\" he reminded me of similar words spoken by IRE's executive director Brant Houston. At NICAR boot camp, Houston repeatedly warned journalists to do the legwork and check for the story behind the numbers. I chuckled as the union president spun his damage control. My computer-assisted reporting training had helped produce an ironclad three-part series that turned never-seen-before records into a powerful expos\u00e9 on teachers' sick-day abuse and its impact on students. I found data processing errors in nearly every file I received from the School Department It's always easy when it's over, but as an editor-turned-CAR reporter I faced the constant pressure of an editor breathing down my neck to get the project done. After every computer analysis update he would declare, \"OK, you've got the numbers, can't you write it now?\" But the numbers were only the lead-in to the story. The numbers, in fact, were a Rosetta stone. As I deciphered their meaning it became clear there was more to teacher absenteeism than just sick days. I had to examine budget issues, the contract, sick leave, accumulated sick leave (teachers can pile up a bank of unused sick days from year to year), and reasons for the absences. Only then could I put into context the statistics from 20,000 Lowell School Department records that threatened to overwhelm me. It didn't help that I was doing this project at home after my regular workday, since the newsroom lacked the necessary computer and software resources. Moreover, my editor's frustration grew as I put more energy into the CAR investigation. The genesis of the project came in casual conversation with the Lowell City auditor in January. He said he was processing 70 to 100 paychecks a week for substitute teachers - the most in his two decades on the job. I then filed a Freedom of Information Act request, one of four FOIAs involved in the project, for the substitute payroll accounts. I found that for 1997-98, Lowell had spent $1.2 million on substitute teachers. I was curious, and that's when I began examining the system of teacher absences. Three months into the project, when I began reporting, I commandeered two general assignment reporters, Jennifer Fenn and Heidi Perlmann, to help conduct interviews and write sidebars. Fenn uncovered 3,000 classroom absences caused by professional development workshops that the School Department had failed to document. Along the way, I adopted a mantra that aided my reporting and writing and led me to the promised land of publication: how does the system work? Titled \"Absent With Leave: Who's Teaching Your Child Today?,\" the three-part series took four months to complete and chronicled teachers' sick-leave abuse and a union contract that helped fuel it, the lust for professional development grant money that literally pushed teachers out of the classrooms and an overall lack of administrative oversight. It concluded that teacher absences were straining the Lowell Public School System financially and taking a toll on the educational opportunities of its 16,000 students. 'Rithmetic The investigation covered the 1997-98 Lowell school year, which was the most recent year for which statistics were available. The major findings were: Lowell's 1,138 teachers had accounted for 16,224 classroom absences, for an average of 14 days each over the 180-day school year. The highest teacher absenteeism due to sick leave occurred on Mondays and Fridays. The days before and after holidays and school vacations also reflected high absentee rates. Teachers called in sick two and three days in advance of their purported illness. Attendance records received little or no School Department scrutiny. Professional development workshops for teachers accounted for more than 3,300 documented classroom absences, with as many as 60 teachers out of the classroom on any given day. The School Department paid out $3.2 million in lost wages to teachers and was forced to hire 10,540 substitutes at a cost of $1.2 million, or $215,000 more than was spent to purchase textbooks for the entire school system. The School Department couldn't keep up with the demand for substitutes, with 1,433 classes going uncovered due to teacher absences. This forced students to double up with other classes, oftentimes eliminating their original course of study, and furthered a dilution of regular instruction. Writing I tried not to make the investigation a numbers story, and I got help in putting teacher absenteeism in perspective by reading articles from the ERIC Database. The writing was reduced by 11 illustrative charts, all but one compiled by using Excel. The most compelling, however, was a full-page calendar of the school year showing the daily breakdown of teacher absences for sick leave, personal days and professional workshops. This was done through Access. A fever chart that revealed the spikes of absences on Mondays and Fridays, as well as holidays and vacations, accompanied the graphic. I showed a preliminary version of the calendar graphic to all my interview subjects, and it was effective in getting them to comment. They couldn't duck what they saw in black-and-white. The first part, published June 13, ran 100 inches and highlighted the patterns of abuse associated with teachers' sick leave. It had a sidebar disclosing that Lowell teachers had accumulated more than $21 million in unused sick-day pay which was an uncompensated liability facing the city. By contract, the city was required to buy back the sick days at 33 percent of their value when teachers retired, meaning taxpayers were on the hook for a budget-busting $7 million. As an editor-turned-CAR reporter I faced the constant pressure of an editor breathing down my neck to get the project done. After every computer analysis update he would declare, \"OK, you've got the numbers, can't you write it now?\" Part Two focused on reaction from School Department officials and the union president, offering details on how the teachers' contract had tied the hands of principals to deal effectively with sick-leave abuse. The contract allowed each teacher 15 paid sick days and two paid personal days a year without excuse. Part Three dealt with the professional development program, which was largely built on state and federal grants. It uncovered the School Department's neglect in evaluating the effectiveness and efficiency of the workshops teachers were attending; $17 million in grant money had forced school officials to adopt a \"use it or lose it\" philosophy. Research The teacher absenteeism investigation was my first in-depth computer-assisted reporting project since my January \"graduation\" from NICAR boot camp. As a senior editor who hadn't authored a major piece since the Reagan administration, the project tested my research and writing skills, both of which were rustier than Noah's nails. But my CAR convictions were greater than my doubts and deficiencies. Instead, the biggest obstacle I faced was my own skepticism as to my self-taught computer skills. As a CAR novice I didn't trust my computer skills. I agonized over making a mistake that would taint the analysis. But I proceeded slowly, beginning the first week in February and working 2-3 hours a night for three months. Overall, I spent 90 hours researching 20,000 School Department records, both electronic and paper. The computer records amounted to four disks stored in three different software programs, none of which matched up with a common identifier for a clean import and merge. The key information - a transactions file for every documented teacher absence - was stored on an old Wang VS system which I got converted to an ASCII file. I found data processing errors in nearly every file I received from the School Department, the most glaring involving 1,014 teachers who were inadvertently listed as missing school for a religious holiday when they were in the classroom. In the end, I had to build a database in Access to contain the information I needed. I also relied heavily on Excel to build an updated assignment roster that gave me the whereabouts of every teacher in Lowell's 28 public schools and what they taught. This helped me later when I cross-checked abs\u00eances by individual schools and by teacher specialization. It led to two explosive charts, one ranking the schools by absences and the other ranking teachers as to their specialization (fourth-grade teachers averaged 17 sick days a year, the most of any group). Each night I printed out reams of analysis and brought them to the office the next day where two newsroom secretaries and available copy editors checked them. If something didn't add up, that night I was back on the computer trying to figure it out. Results There was no shortcut to the final conclusions. I had to study the teachers contract, pester the city's chief labor negotiator and interview two dozen school officials, including administrators from outside Lowell who offered valuable insights as to what patterns of abuse I should be looking for. In addition, I had to dispel my editor's perception that I was punching computer keys without a sense of purpose. In the end, we both learned that CAR projects require trust and patience. James Campanini can be reached by e- mail at jcampanini@lowellsun.com",
            "headline": "\"Sick\" Slackers",
            "author_name": "JAMES H. CAMPANINI",
            "author_title": "reporter",
            "month": "September",
            "year": 1999
        },
        {
            "full_text": "Bernard Hedge was screwed. I knew it the moment I laid eyes on him. The former bodybuilder was stuffed into a wheelchair in a two-room apartment with barely enough room for him to maneuver. His kidneys were failing. His blood pressure and diabetes were uncontrollable even with the dozens of pills he took each day. \"Life is not guaranteed for any of us,\" Hedge told me. \"I could live another year, five years, 10 years. Nobody knows.\" Within a few months of the publication of my stories on the gap in death rates between whites and blacks, he would lose a leg to the diabetes. Then, a few months later, like too many black men, he'd lose his life early. He was 55. Death rate data I had decided to examine the gap in death rates for preventable and treatable diseases after a nurse friend introduced me to Hedge and after hearing Dr. Patrick Remington, an epidemiologist at the University of Wisconsin School of Medicine, speak at an IRE medical reporting conference. Hedge was screwed, but he wasn't pathetic. His struggle to finish his life with dignity impressed me. Our stories found that black men were dying from these diseases in astonishing numbers.. rate nearly three times greater than white men. Remington introduced me to an invaluable resource for mortality data - the Centers for Disease Control and Prevention online \"Wonder\" system (http://wonder.edc.gov). With it, I was able to calculate death rates by race, gender, and disease for every county and state in the nation. With it, I learned just how screwed Bernard was. After experimenting with Wonder, I decided to look at death rates for people under 65 years old suffering from seven treatable or preventable diseases. The idea was that these diseases - female breast cancer, cancer of the cervix and uterus, pneumonia, prostate cancer, colorectal cancer, high blood pressure and diabetes - might be a crude measure of people's access to health care, or lack of access to it in this case. Our stories, published by The Plain Dealer in May 1998, found that black men were dying from these diseases in astonishing numbers, with more than 140 deaths for every 100,000 black men, a rate nearly three times greater than white men. The death rate for black women was double that of white women. For some of the illnesses, the death rate for blacks was now where it was for whites two decades ago. And the gap between the races was widening. Pursuing prejudice A short time after my stories came out, I was blown away by a five-day series by Ford Fessenden - then of Newsday and now of The New York Times - that examined many of the same issues, but in much greater detail. Ford had attended the same IRE medical reporting conference I had. His eight-part series, \"The Health Divide,\" published in Newsday in November and December of 1998, is available from IRE and is a must-read for anyone interested in health care reporting. For my stories, I also used death certificate data that I purchased for $100 a year from the Ohio Health Department. I found these records (one for every person who died in the state that year) useful for finding relatives of people who died of specific illnesses. In the past, I've also used them to analyze C-section rates at hospitals across the state, find premature babies who died in hospitals not properly equipped to care for them, and uncover deaths at dangerous heart cath labs, among other things. According to the National Center for Health statistics, 19 states make death records publicly available. They are California, New Jersey, Connecticut, Florida, Georgia, Iowa, Kentucky, Maine, Massachusetts, Michigan, Minnesota, North Carolina, Ohio, South Carolina, South Dakota, Vermont, Washington and Wisconsin. Most of them also make birth certificate records available, too. For all but eight states, some restrictions apply or state officials will make you jump through some hoops to get the data, such as producing a research proposal on how you'll use the records. But I think you'l find the records worth the trouble. Vital Resources for Vital Statistics: National Center for Health Statistics: (301-436-8500). A great resource for reporters working with vital statistics data. National Association of Health Data Organization: (703-532-3282). Keeps tabs on which states make available vital statistics and hospital discharge data and how the public can get it. Centers for Disease Control and Prevention's online \"Wonder\" data system: (http://wonder.cdc.gov). Allows you to run queries on mortality data for counties, states, or the nation. You can e-mail the results to yourself and then import the data into a spreadsheet for sorting and making nifty graphics. Dave Davis can be reached by e-mail at ddavis@plaind.com",
            "headline": "Healthcare by color",
            "author_name": "Dave Davis",
            "author_title": "null",
            "month": "September",
            "year": 1999
        }
    ]
}