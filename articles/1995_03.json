{
    "stories": [
        {
            "year": 1995,
            "month": "March",
            "headline": "Beware examining FBI data - When crime doesn't add up",
            "author_name": "Gwen Carleton",
            "author_title": "NICAR Staff",
            "full_text": "Crime statistics are full of potential disasters. Just ask Shawn Macintosh of the Dallas Morning News. 'Several years ago, I found a ring of 9-year-old kiddie crack dealers in Macon, Georgia,' she recalled. But Macon police said there was no problem. And it turned out they were right. The kids were fabrications, the result of data entry mistakes. In the past, many reporters left statistical analysis up to the experts. Today, more journalists are analyzing the data themselves, and are producing sophisticated stories about crime rates and trends as a result. But they also are finding just how difficult it can be to get the story right. The FBI's Uniform Crime Reports are among the nation's largest and most popular crime databases. The collection of six databases, compiled and released annually, provides data on the victims, offenders and circumstances of millions of crimes committed nationwide. The FBI publishes 'Crime in America,' a book-length analysis of the year's statistics, shortly before it releases the raw data each year. The Uniform Crime Reports have a number of strengths, among them size, detail and a 66-year history. But there are weaknesses as well. The FBI gathers its data from local police, then checks the numbers for mathematical errors and unusual fluctuations. But it doesn't catch every mistake. In addition, participation in the program is voluntary - and not every town volunteers. 'It's a 'we take your word for it' situation,' said Carol Napolitano, computer-assisted journalism coordinator for the Munster (Ind.) Times. 'Often, there were significant discrepancies between what we got from the FBI and individual agencies.'"
        },
        {
            "year": 1995,
            "month": "March",
            "headline": "The case of the unknown stranger",
            "author_name": "David Royse",
            "author_title": "University of Missouri",
            "full_text": "Run for your lives! Everyone in the United States has a realistic chance of being murdered! After all, the FBI said so in December. In Crime in the United States, 1993, a summary of the Uniform Crime Reports, the agency said the statement 'is somewhat supported by the fact that a majority of the nation's murder victims are now killed by strangers or unknown persons.' In other words, random murders are soaring. Scary stuff. Numerous newspapers and wire services ran with the FBI's assertion. But the claim is not backed by the numbers. That is because there is a big difference between a stranger and an unknown person. The distinction brings up a crucial rule, regardless of whether a reporter is running with a press release on a tight deadline or sifting through massive amounts of data: Know your definitions. The difference between a stranger and an unknown person is enormous. A stranger, according to police definitions, is someone the victim did not know. An unknown offender, on the other hand, means the case is unsolved. They don't know who did it. It could have been the victim's mother. Or it could have been a friend. Or a stranger. There are many aspects of FBI data that journalists should clarify for their readers. The database represents only reported crime; actual crime is higher. Statistics on violent crimes are more reliable and uniform than those for lesser crimes. Some crimes, such as spousal abuse and child abuse, are not included at all."
        },
        {
            "year": 1995,
            "month": "March",
            "headline": "Computer records reveal environmental havoc",
            "author_name": "Michael Fabey",
            "author_title": "Journal of Commerce's New York bureau",
            "full_text": "When local resident Jim Pickron contacted the Fayetteville (N.C.) Observer-Times, we figured the most that would come about was a human-interest piece about the dangers of careless crop dusting. Instead, using computers, we found the story was about a system gone wrong - that government regulators don't do their job when it comes to watching those who spray pesticides. Pickron was putting up a fence in his backyard when he was accidentally sprayed with chemicals. The state fined the pilot, who admitted his error. We wanted to find out whether Pickron was the victim in an isolated incident. We first asked the state for similar cases. They gave us a printed list of about 200 names of those who misused pesticides in the last two years, plus the victims, the fine, the violation code, and a little more. I was beginning to learn about computers and wanted to compile the information into a spreadsheet. The newspaper installed Paradox into the IBM 386 on my desk. I typed for a day and a half. And I noticed some information was missing, such as the full name of complainants and violators. I went to the state Pesticide Division in Raleigh, and the clerks let me check some of the records in their computer. Their database included more than 50 fields of pertinent information, from types of chemicals to the extent of injuries. In addition, the database included twice as many records as the printout they first gave me. It turned out the printed version they gave me did not include cases where they had taken little action. I asked for a copy of the database on diskette. The agency balked, claiming they did not have authority to release it since some of the fields were not considered public record. I cited state open records law and told them they were required to turn over a version of the database with all fields open for public inspection. They refused. I called the Pesticide Division administrator and said I was working on a story about how the agency was violating state law by withholding information. I got the diskettes the next day. The database was invaluable. We used Paradox and found repeat offenders, the most common place for violations, and crop dusting that resulted in injuries. Then we used Paradox to print mailing labels - and we sent surveys to every person who filed a complaint in the last two years. The survey showed people believed the agency did not treat them well. We also got victims to tell their stories, vital for the project. The result was a five-day series that examined how local, state, and federal regulators often failed to ensure the safety of the likes of Pickron. The articles also showed that those who endangered people and the environment often went unpunished."
        },
        {
            "year": 1995,
            "month": "March",
            "headline": "Elevator inspections show bad rides - Go up the database staircase",
            "author_name": "David Armstrong",
            "author_title": "Boston Globe",
            "full_text": "Every day, millions of people use elevators and escalators to travel to work, shop, or attend a sporting event or concert. Most people, however, give little thought to the safety of these machines. The Boston Globe recently completed a three-part series that examined the elevator and escalator industry's inattention to safety and the frightening ramifications for the riding public. One of the first things we requested was a copy of the state's database of elevator and escalator inspections. The database was created only two years earlier, replacing a card catalog system that had been in place since Mr. Otis invented the first elevator years ago. The card catalog system was a reporter's nightmare: a paper search that would have required perhaps hundreds of hours of work. The computerized records, however, allowed us to quickly and conclusively evaluate the state's ability to inspect the 30,000 elevators and escalators. Perhaps unaware of the potential manipulation of its own data, the state graciously agreed to provide us with a copy in dBase format of its inspection file only several days after it was requested. Using FoxPro, we determined that nearly 40 percent of the state's passenger elevators had not been inspected as required. Even more revealing was the fact that of the elevators inspected, one-third flunked the state's safety test. And of those flunking the safety test, only two percent were shut down until mandated repairs were made. By using the 'group by' command, we could also see there was a wide disparity in inspection rates from city to city. In Worcester, the state's second largest city, nearly 75 percent of the elevators and escalators had not been inspected as required. We created a chart of the inspection rate by community and imported it to MapInfo to produce a map. This ran on the second day of the series and allowed readers to pinpoint the inspection rate in their community. The database also helped us identify inspectors who were not doing their job. The database provided the address and date of each inspection performed by inspectors. There was a wide disparity in the workloads. This information was helpful in identifying inspectors who would later be observed by Globe reporters. The Globe found numerous inspectors goofing off on the job, spending their work days at home or running errands. Others worked two jobs. Seven inspectors and supervisors were suspended without pay after the Globe series was published. The two top officials in the elevator inspection office were demoted. And the state public safety official ultimately responsible for elevator inspections resigned from office two days before the series first appeared. The database also allowed us to say many inspectors appeared to be doing their job, judging by the number of inspections they completed. The database of inspections was used as a continuing resource during research for the series. We could quickly check the status of particular buildings or accident sites. We even checked the status of elevators and escalators at the Globe. We found out two of the elevators had not been inspected. This information was reported in a sidebar to one of the many follow-up stories."
        }
    ]
}