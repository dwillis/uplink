{
    "year": 1999,
    "month": "November",
    "headline": "Kids and Violence: Behind school walls",
    "author_name": "John Kelly",
    "author_title": "The Associated Press, Indianapolis",
    "full_text": "Newspaper headlines and nightly news teasers delivered the good word: fewer kids were caught bringing guns into America's schools. The news stories cited a federal survey of the states, which reported a drop in gun-related expulsions during the 1997-1998 school year. The timing was bizarre for me in Indianapolis. For months, I had been eyeballing a similar school discipline database I knew the Indiana Department of Education was compiling. Just days before the national report came out, I finally got the disk with the Indiana schools' data. One dataset listed every expulsion at every public school along with fields showing the student's grade, gender, date of birth and the reason for expulsion. Another tallied the number of suspensions by reason for each school. We were going to be able to do exactly what I had hoped: provide parents with a school-by-school accounting of the discipline happening behind the local schoolhouse walls. Within days, we gleaned from the database how many students were kicked out of each public school for bringing a handgun to campus, using drugs, alcohol or tobacco or just acting out. Then, we went to work reporting. The data was easy to confirm and hard for school officials to dismiss. That's because it comes straight from the local principals' offices. Each school year, every public school in Indiana must file paper reports with the Indiana Department of Education documenting their disciplinary actions-some of which is tallied for an annual report the states must submit to the U.S. Department of Education under the Gun-Free Schools Act. The paper reports are certainly public records. Better yet, Indiana keys information into a database. Other states probably do the same. In our analysis and reporting, we found some weaknesses in the data. First, the tallies for each school only showed how many times students got caught and punished for bringing guns to school or breaking other rules. There is no way to know how often offenses actually occurred on campus. Some schools may crack down on certain offenses while others turn a blind eye. Second, like most crime data, you are going to find inconsistencies when you ask hundreds of school officials to lump incidents into broad categories based on vague definitions. In Indiana, for example, one school confiscated what looked like a grenade from a student and promptly expelled him, state officials said. The grenade turned out to be a cigarette lighter, but the school reported the expulsion to the state under the category of 'other firearms.' We overcame those weaknesses with reporting. We talked to state and local educators, students and others. We also carefully studied the paper reports that schools filed and the instructions they were given in filling out those forms. As with most CAR stories, there was more traditional reporting involved here than number-crunching. The whole project took less than a week from the day I popped the floppy disk into my computer. All offered insight that helped us explain what conclusions people could and could not draw from the figures. Interviews with school principals gave us added opportunities to check our data against the paper reports they had filed with the state. We checked many schools' figures against individual reports provided by the state. Along with our stories, we filed charts listing the expulsions and suspensions at every public school in Indiana for various offenses. That allowed local newspapers and broadcasters to do their own stories. We filed the package within days of the federal report's release and just as Hoosier children were headed back to school. Along the way, we uncovered a problem with the national report: it was at least partly wrong. Indiana had reported 62 gun-related expulsions to the feds. But our database included 129 gun-related expulsions. We rechecked our database work, and I called the state Department of Education official in charge of collecting and reporting this data and asked why the totals were different. He called back to say the state totals were right and the ones in the federal report were wrong. We continued working on our stories, while the state education staff met to figure out what went wrong. The official called back to say the state had under-reported their figures and said, 'We just have to own up to this one.' Education officials in other states reported discrepancies in gun expulsion figures too, and the AP moved a national story highlighting the national report's errors. As with most CAR stories, there was more traditional reporting involved here than number-crunching. The whole project took less than a week from the day I popped the floppy disk into my computer. We got the data for free, but Indiana's education department is accustomed to forking over large databases to us for nothing. Once we had the database, the computer work was no more sophisticated than importing a text file into Microsoft Access and running a half dozen sum and count queries. Reporters in other states, which collect and compile the same data from local schools, could easily replicate the reporting."
}