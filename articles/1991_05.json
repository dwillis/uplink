{
    "articles": [
        {
            "year": 1991,
            "month": "May",
            "headline": "Transporting Toxics Through California",
            "author_name": "Christopher Schmitt",
            "author_title": "San Jose Mercury News",
            "full_text": "Cradle to grave. To spare the environment and safeguard public health - that's the way toxic waste is supposed to be monitored, from when it's created until it safely meets its demise. On paper, environmentally conscious California has one of the toughest systems in the nation for keeping track of toxic waste. But in a recent four-part series that drew heavily on computer-assisted reporting, the San Jose Mercury News documented big problems in monitoring and management of toxic waste: - California's system for keeping track of 2 million tons of waste a year is riddled with so many holes that it can't account for where hundreds of thousands of tons of waste go each year. - Although notoriously illegal dumpers might be caught with the help of some of the tracking information, regulators have a policy to ignore tipoffs the system offers. - The biggest violator of waste disposal reporting rules is the state itself. - Recycling of toxics often is a sham, nothing more than disposal in disguise. - Waste generators have roundly failed in efforts to cut down the torrent of waste they produce - even though there's clear evidence such cuts can be made profitably. Key to making our project work were 410,000 computerized records detailing waste shipments for 1989, the latest year for which data were available. These records, not to be confused with TRI [3], were from shipping documents known as manifests. They offered a load-by-load look at waste disposal. This information created big databases - one with about 60 megabytes, another with 30. To slice through it all, we used a 33-megahertz, 80386-based IBM PC clone with a 360 MB hard disk drive. (When we were done, we had pretty much filled up the drive.) We didn't use the MICAR-touted XDB database management software, instead opting for Paradox version 3.5. We can confirm recent highly favorable computer magazine reviews of Paradox: It's quite speedy and offers excellent, easy to learn Query-By-Example system. \"When we matched a list of Superfund sites against waste destinations, we identified dozens of tons of waste that had been illegally shipped.\" Some of the most productive computer work on this project came in matching one set of data against another. California has a law that prohibits transport of hazardous waste on state highways if the destination is a federal Superfund site. When we matched a list of Superfund sites against waste destinations, we identified dozens of tons of waste that had been illegally shipped. When we asked one of the biggest offenders about their practice, they immediately stopped. We also used matching to dispute state officials' claim that their tracking system didn't offer information of value for cracking down on toxic violators. We took a sample of 36 enforcement cases in one year, then looked back to the previous year to see whether the tracking system had pointed out any problems with these firms. It had - 4,978 different times. In one case, we found that one of the biggest hazardous waste cases in years - involving smuggling of waste into Mexico - could have been cracked months earlier if regulators had paid attention to tracking system information. You can do what we did, but outside California, information may be more limited. For any questions about what we did, please feel free to call: (408)-920-5048."
        },
        {
            "year": 1991,
            "month": "May",
            "headline": "C.P.U. Time, Part Two",
            "author_name": "Tom Braden",
            "author_title": "MICAR",
            "full_text": "Last year the Dayton Daily News asked the state's Department of Motor Vehicles to provide computer tapes of drivers license records. The tapes contain about seven million records. The state agency said it would provide the tapes at a cost of $3 per record - a mere $21 million. Similarly, the Hartford Courant recently sought death certificate records from the state's Department of Health and Human Services. The agency wanted $10,000 for the records. These are just two examples of a trend that is occurring with increasing frequency throughout the nation: Government agencies, suddenly besieged by journalists seeking computer tapes, are finding ways to keep information from the public by charging absurd amounts for making electronic copies of information. This means reporters need a way to analyze costs and determine if public record custodians are gouging them with outrageous prices. Government agencies using mainframe computers generally use Central Processing Unit (CPU) time to allocate costs. A mainframe computer's CPU performs all the computer's data movement. Agencies charging CPU time do so because it allows them to perform several different jobs at once. These agencies use operating system software that have special utilities, such as accounting programs, to account for and charge different users. To recover the costs associated with operating a mainframe computer, government agencies and data centers first identify these costs, then make a volume estimate, using CPU time, on how much the mainframe is used in a year. They divide the volume estimate into the cost to come up with a rate. This process, called computer cost accounting, allows the mainframe operator to take into account all of the overhead costs, such as the cost of the mainframe itself, labor, utilities and rent, in establishing rates for users. An organization quoted a price of $600 per minute of CPU time, which actually occurred in Boston, can use the agency's own formula to determine if the rate reflects the actual cost to the agency. One would multiply $600 by the number of minutes in a year -- 525,600. Multiplying that figure by $600 gives you $315 million. The question then becomes, how does that match the agency's known operating costs? The news organization requesting computer tapes must match the agency's quoted cost figure against the amount of CPU usage in a given year just as the agency itself does in allocating costs. If the mainframe is active, say, 50 percent of the time, obviously, it does not cost the agency more than $157 million to operate its system in a given year. Journalists must not acquiesce to the outrageous fees for duplicating computer tapes. Case law involving the cost of such records remains scant. But as news organizations accelerate their pursuit of public records stored in electronic format, the number of cases going to court is sure to increase. What is desperately needed is some kind of uniformity among states as well as among different government agencies within a given state as to what electronic records should cost the public. To use computerized records as a means of increasing state revenue obviously contradicts the intent of freedom of information laws, which is to ensure the public access to government information so that it may exercise its duty of self-government."
        },
        {
            "year": 1991,
            "month": "May",
            "headline": "Using Data at the Nation's Data Capital",
            "author_name": "Karl Ross",
            "author_title": "MICAR",
            "full_text": "There is no greater concentration of computerized information than in Washington, D.C. But strangely, in this capital of government record keeping, there seems to be a lack of computer-assisted reporting. Scripps-Howard Bureau Chief Al Thompson says that although he senses a growing interest in accessing government computer files, computer-assisted reporting is still a marginal technique at his office. \"We have our hands full just doing what we're doing day-to-day putting out the news, so we don't have a lot of time to discover what's out there and what's available,\" Thompson says. Rich Morin, who heads up the computer-assisted effort at The Washington Post, suggests that because most news organizations in Washington, D.C., are bureaus for out-of-state papers and national magazines, they are geared toward beats using more traditional means. \"There's quicker ways to get some of this information,\" he says, \"though certainly not all of it.\" Tom Moore, formerly a Washington-bureau reporter for Knight-Ridder and now a graduate student at George Washington University, offers a more sociological explanation. He says that for many reporters, a position at the Washington bureau represents a hard-won plum, topping off a successful career in the newsroom. Most Washington bureau reporters, Moore says, are older, more prosperous and more at ease using conventional reporting techniques. \"There are exceptions,\" he says, \"but the last time I looked, reporters in Washington were very far behind in information technology.\" Most major news organizations have appropriate technology at their disposal but don't employ it. Knight Ridder Bureau Chief Clark Hoyt says his office is equipped with computers and they try to keep a running database on campaign finances but they don't have anybody to work with these resources on a full-time basis. If computer assisted reporting has yet to realize its potential in Washington, Hoyt says, \"It's not because it's not a good tool. It's because we as an industry haven't taken full advantage of it. Cox Newspapers' Andrew Glass wants to change this. Cox recently invested in a CD-ROM player for its Washington bureau and has taken a more aggressive approach to the technology. \"It's something we feel the time has come for,\" Glass says. He says that instead of hiring some computer-jock to handle the chores for the office, he would like to see all staffers get involved to the extent that they are able. \"We think it should be part of everybody's job, just like knowing how to type.\" Even though a multitude of think-tanks and public interest groups are providing the media with their own ready-made analyses of government files, Glass questions the sagacity of relying on such \"canned\" sources. \"It's garbage-in, garbage-out if you're not sure how you came by the data,\" Glass says. Another Washington-based organization that is counting on computers to give it a jump on the competition is Gannett's USA Today. The special projects department created a special computer-assisted unit in 1988. Julia Wallace, head of special projects, says the investment has yielded a number of significant stories from dangerous roads to troubled banks. The special unit has been especially busy cranking out customized reports comparing 1980 and 1990 census data. Wallace warns newcomers that it is not as easy as it seems. \"Logically, you think you ought to be able to press a button and get the answer right away,\" she says, \"but the reality is things take twice as long and are twice as hard as you expect them to be.\" \"But we're at the point where we've been through those learning stages. It's a lot more fun because now we're able to do things a lot quicker and things that are a lot more complicated.\""
        }
    ]
}