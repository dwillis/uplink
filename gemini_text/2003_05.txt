==Start of OCR for page 1==
Uplink
May • June 2003
Volume 15, Number 3
Published bimonthly by the National Institute for Computer-Assisted Reporting
www.nicar.org

RELIGION
Homemade files track priests
By Jaimi Dowdell
IRE and NICAR

Many journalists who recently investigated sexual misconduct within the Roman Catholic Church faced a major hurdle: a lack of data about allegations of abuse by priests. Some news organizations found the answer to this problem by creating their own databases. The stories that resulted show how useful these homegrown databases can be.

In January 2002, The Boston Globe began publishing "Crisis in the Catholic Church," a series that offered a picture of a Catholic archdiocese riddled with scandal. More than 800 stories later, The Globe's series has garnered many awards including a Pulitzer Prize for public service and the IRE Medal. Conducting an intense investigation into the church's private realm, the newspaper's Spotlight Team found success through the use of shoe-leather reporting, documents and a database that greatly supported the re-
continued on page 20

CAR FOR WARTIME
Finding backyard military contractors
By Adam Bell, The Charlotte Observer

With war looming in Iraq and the demand for related local news stories rising a few months ago, we decided to look at which companies in the Carolinas benefited from military contracts.

We obtained the Federal Procurement Data System from the IRE and NICAR Database Library and imported it to Access 2000 for our initial queries. Because this database compiled by the U.S. General Services Administration contains all federal contracts with a value of more than $25,000, we needed to filter our queries to limit our results to Department of Defense contracts. Then we decided to omit contracts granted by the civilian Army Corps of Engineers since the story focused on military activities.

Now we were ready to explore the data two ways: for work performed in the Carolinas and work performed by contractors based in the Carolinas. We decided that it was more interesting to
continued on page 22

SPOTLIGHT: RACE
Segregation in black white, shades of gray
By Holly Hacker, IRE and NICAR

What is the most segregated metropolitan area in the United States: Detroit, Milwaukee, Minneapolis or New York?

They all are. It just depends on how you measure segregation.

And there are lots of yardsticks to choose from. Researchers have devised at least 20 ways to measure residential segregation. That's great for sociologists who have months to write detailed articles for scholarly journals, but not for journalists who must pound out stories on deadline for a general audience.
continued on page 12

SPOTLIGHT:
FOR MORE ON RACE SEE
• Analysis finds search rate disparities in San Antonio, p. 8
• Toronto arrest stories ignite firestorm, p. 10

==End of OCR for page 1==

==Start of OCR for page 2==
PE Datase Librery 573.884.7711

Bits & Bytes

Data
The public is becoming increasingly aware of the risks associated with obesity. A study published in April in The New England Journal of Medicine reports that being overweight increases the risk for death associated with cancer. In 2000, more than 1,000 people died in the United States because of causes directly related to obesity, according to the Mortality, Multiple Cause-of-Death database recently added to the IRE and NICAR Database Library collection of federal government databases.

Journalists can use this database to track many diseases and other health issues, such as infectious disease and cancer, within their state or community. The data consist of detailed information about each death compiled from death certificates. Geographic information is complete for metropolitan areas with populations of 100,000 or more.

The 1988 through 2000 data include death certificate records for the entire United States and U.S. territories. Detailed demographic information about the deceased, including age, race and marital status can be determined. Causes of death can also be analyzed by using the International Classification of Disease (ICD) field. Specific details about each death are also included, such as whether it was
continued on page 4

CAR practitioners share tricks
By David Herzog, University of Missouri and NICAR

More than 250 journalists from across the globe gathered in Charlotte in March for IRE and NICAR's Annual Computer-Assisted Reporting conference. Attendance was smaller than many of the other recent CAR conferences, but those who attended were part of a committed bunch.

How committed? Let me put it this way: Some of the conference-goers helped load CDs full of software onto the PCs in training rooms. Others performed manual labor, hauling PCs and monitors out of packing crates and lugging them into the training rooms. It was a great reminder that IRE and NICAR work because the journalists at the grass roots work by sharing their time, experience and knowledge.

For those of you who missed it, the conference was packed with panels and demo sessions on all kinds of topics: weaving CAR into the beat, data for covering CAR during wartime and spatial analysis for investigations. And then there were special advanced workshops geared to CAR power users and hands-on classes for everyone. With the prospect of war looming, many of the panels had a special focus on defense and homeland security. Likewise, the spotlight panel focused on a timely subject: the battle over access to information.

If you want to catch up on the happenings at the conference, you can order tipsheets provided by panelists. For more information about ordering contact the IRE Resource Center at (www.ire.org/resourcecenter or 573-882-3364). The tipsheets are a great way to learn lessons from other journalists.

Thanks to everyone for making this a great conference.

Contact David Herzog by e-mail at dherzog@nicar.org.

CAR training upcoming

IRE and NICAR has numerous training opportunities in the coming months for journalists who want to learn computer-assisted reporting. For a complete list of training events visit www.ire.org/training/otr.html.

Here are some of the highlights:

Mapping Data for News Stories, July 25-27 in Columbia, Mo., is for journalists who want to extend their CAR skills by using geographic information systems (GIS) to enhance their reporting. Trainers will be Jeff Porter and David Herzog of the NICAR staff, both longtime users of GIS in news reporting. Herzog is the author of "Mapping the News," a case study book published by ESRI Press that shows how journalists use GIS.

Journalists can get an overview of computer-assisted reporting at Better Watchdog Workshops offered by IRE and the Society for Professional Journalists. Upcoming workshops include Sept. 11 in Tampa, Fla., Oct. 4 in Eugene, Ore., and Oct. 25 in State College, Pa.

2
May June 2003

==End of OCR for page 2==

==Start of OCR for page 3==
IRE AWARDS
CAR fuels TV, other division winners
By Rachel Schaff, University of Missouri

IRE received a flood of entries for its 2002 investigative reporting contest. Many of the entries used computer-assisted reporting, along with traditional reporting techniques. The following is an overview of some of the winners and finalists that used CAR in the television, other media and special categories.

Television
Top 20 Market – Finalist
WFAA-Dallas: "Fake Drugs, Real Lives" by Brett Shipp, Mark Smith, Kraig Kirchem and David Duitch. This investigation examined the legitimacy of drug busts made by Dallas police. The journalists found that nearly half of the police department's cocaine seizures contained little or no illegal drugs. (Story No. 19889 in the IRE Resource Center).

Instead, the substance seized was gypsum, a main component of billiard chalk. Further, the arrests often involved recent Mexican immigrants. The reporters developed a Microsoft Excel spreadsheet of Dallas County felony drug cases for 2000 and 2001. Reporters used the spreadsheet to separate defendants with Hispanic surnames and determine whether the seizures involved illegal drugs.

The story has won many other awards, including a George Foster Peabody Award; an Alfred I. duPont-Columbia University Silver Baton; a Harvard University Goldsmith Award Finalist for Investigative Reporting; Texas Gavel Award; the Katie Legacy Award; and Katie Award for Television Investigative Reporting.

Below Top 20 Market - Certificate
WTVF-Nashville: "Friends in High Places" by Phil Williams and Brian Staples. This seven-month investigation exposed the questionable relationship between Tennessee Gov. Don Sundquist and some state contractors. Journalists used Microsoft Excel to analyze spreadsheets of all noncompetitive contracts awarded by the state of Tennessee. (Story No. 19649)

This story has also won a George Polk award, a National Headliner Award and a Sigma Delta Chi award.

Below Top 20 Market - Finalists
WITI-Milwaukee: "The Pain Doctor" by Bob Segall and Diane Carbonara. This investigation focused on a local doctor who used questionable medical and billing practices and acted abusively toward his patients. Journalists used CAR to analyze information acquired through interviews with dissatisfied patients and to analyze pharmacy prescription data to determine the amounts and types of medications prescribed by the doctor. "The Pain Doctor" also has won a regional Edward E. Murrow award, Best Series Reporting from the Wisconsin Associated Press, and Best Investigative Report from the Milwaukee Press Club. (Story No. 19674)

WOAI-San Antonio: "Councilman & Judge - A Cozy Connection" by Brian Collister, Joe Ellis and Stephanie Kline. This piece investigated a San Antonio criminal defense attorney (who is also a city councilman) and his relationship with a female judge. The journalists used Microsoft Access and Excel to analyze databases of criminal court records and county payment records. They determined which attorneys received the most court appointments and made the most money, both in each court and across all courts. Eventually the results helped show that the councilman and the judge had a romantic relationship, shared a bank account, shared an address at one time, and possibly committed fraud together. This story received an Honorable Mention in the Investigative category from Texas Associated Press Broadcasters and also won first place for Best Continuing Coverage by the Texas Associated Press Broadcasters. (Story No. 19671)

Other Media
Magazine/ Specialty Publication - Finalists
Time Magazine: "Look Who's Cashing in at Indian Casinos" by Donald L. Barlett and James B. Steele. This investigation revealed that Indian reservation casinos do not benefit the majority of Native Americans; most of the proceeds go to non-Indian investors and a handful of small tribes with fabulously profitable casinos. CAR played a large role in the investigation; Barlett and Steele made extensive use of databases, the Internet and spreadsheets. One of the most important databases, the Indian Labor Force Report, contains information about individual tribal enrollment as well as the employment rate and poverty levels of each tribe. (Story No. 19517)

U.S. News and World Report: "The New Math of Old Age: Why the Nursing Home Industry's Cries of Poverty Don't Add Up" by Christopher Schmitt. The nursing home industry has long contended that the federal aid it receives is inadequate; in 2002 nursing home operators lobbied the federal government for additional billions of dollars of aid. However, this investigation showed that the nursing home industry is, in fact, profitable and growing, with many nursing homes earning 20 percent to 30 percent profit margins. The investigation required many different databases of nursing home cost reports. Schmitt used Paradox for database management, FoxPro for data import and Excel for spreadsheet analysis. (Story No. 19495)

Online - Finalist
Center for Public Integrity: "Enron Top Brass Accused of Dumping Stock Were Big Political Donors" by John Dunbar, Robert Moore and MaryJo Sylwester. This investigation looked at the col-
continued on page 23

Uplink
May June 2003
3

==End of OCR for page 3==

==Start of OCR for page 4==
Visit our Web site www.ire.org

Bits & Bytes
continued from page 2

an accident, occurred in the workplace or was a suicide.

For more information about this database go to www.ire.org/datalibrary/databases/health.

Training
Learn how ArcView 8 GIS can help you explore data for great stories. The Mapping Data for News Stories mini-boot camp July 25-27 in Columbia, Mo., will provide journalists with instruction on how to combine data with geocoding, thematic mapping, overlay and spatial analysis to enhance their reporting. For more information about this mini-boot camp, including how to register, go to www.ire.org/training/mapJuly2003.html; for information on other IRE and NICAR training and events go to www.ire.org/training/otr.html.

Tipsheets
If you missed the 2003 Annual Computer-Assisted Reporting Conference in Charlotte, N.C., there are still opportunities to learn what other CAR professionals presented at the event. More than 65 tipsheets are available from the conference, providing CAR techniques and story ideas for just about every beat. They include information on Access forms, Census data, spatial analysis, social network analysis and much more. To download the tipsheets, go to www.ire.org/training/charlotte and click on "Tipsheets from this Conference".

CRIME
Database sleuthing uncovers the missing
By Lise Olsen, Seattle Post-Intelligencer

Deep in the heart of Green River Killer country our initial goal was to try to identify probable serial killer victims.

We did that and found a lot more, but not without a struggle. We learned that when someone went missing his or her records often disappeared too.

That was one of the challenges reporters at the Post-Intelligencer faced when we attempted to analyze 20 years of missing-persons reports from more than 300 police agencies in the state of Washington. The police had misplaced or purged dozens of reports for people still listed as missing in the official state and national databases.

The official databases also were inaccurate. Many people who should have been included weren't, and other cases that should have been purged remained in the system. The consequences were dramatic: Murder cases went unsolved, bodies remained unidentified and killers stayed free.

The databases we built and those we obtained from police, coupled with traditional investigative reporting, allowed us to make such sweeping conclusions with confidence.

We also found that agencies had lost or destroyed more than 100 reports for active missing-persons cases. Coroners and medical examiners had lost files, physical evidence and even remains. Most police agencies routinely violated a law that requires them to seek dental records in long-term missing person cases, even though the records can provide key clues in identifying corpses.

The database steered us to dramatic human stories: a foster child whose disappearance and mysterious death in Mexico was never adequately investigated. A toddler, the unidentified victim of murder in 1987, whose image had never been publicized until our series ran. A missing woman whose frantic family could not convince police to take a report after she slit her wrists and drove away.

The anecdotes alone were appalling but our database allowed us to combine the tales with hard numbers. Our investigation took nearly a year, mainly because it was so difficult to obtain records, most of them police reports from open investigations. But it also took time to track down families of the victims.

Building the database
Reporter Lewis Kamb-filed the first public disclosure requests in January 2002, asking all the departments across the state for unsolved missing persons reports. From the beginning, the newspaper specified that it wanted reports that were at least a year old. We didn't want to track the thousands of teen-agers and adults each year who return home after a few days.

Federal and state auditors regularly are supposed to review whether police accurately track long-term missing persons cases in the National Crime Information Center (NCIC) database. But it immediately became clear that most departments didn't have their records in order. While some departments responded to the public records requests within a week or two, most dragged their feet. Some flatly refused to provide records. Others provided information only in summary form or redacted names. It took the Seattle Police Department, the state's largest agency, nine months to respond.

Kamb was relentless in his pursuit of

4
May June 2003

==End of OCR for page 4==

==Start of OCR for page 5==
the records. He pointed out to the reluctant police departments that some major police agencies had immediately provided copies.

After several months, I got involved in building the database. Kamb had boxes full of reports. But, using a list of official missing persons provided by the state, I quickly realized that many departments had not given us everything. We wrote back again and called some departments dozens of times.

By late fall, we had assembled more than 600 reports in all sorts of formats, some handwritten, some barely readable, but most usable. We had hoped to map the locations of suspicious missing-persons cases and compare those sites with the body dump sites of known serial killers. I soon realized that would be impossible: In hundreds of reports, exact locations of where victims had disappeared were unspecified or had been scratched out.

Instead, I built an index to our reports that included fundamental information, such as the department name, date, victim name, age, sex, race, where disappeared (if indicated), usual residence (if indicated), a description of the victim and a summary of circumstances behind the disappearance.

In addition to those fields, I added a couple of more subjective categories that helped us build the stories.

I flagged reports that indicated an unnecessary delay in police investigations. (One of the theories we had as we investigated was that police often "dodged" missing-persons reports). I marked others that seemed to bear more investigation as "interesting" in the database. Later on, we investigated the "interesting” stories and picked out anecdotes that helped illustrate trends.

I also attempted to categorize the reports by the reasons police had given in the reports to possibly explain the disappearance. This was tricky, but it was important to try to cull out reports in which foul play was suspected, since another theory we were trying to check was whether police made enough of an effort to help people whose lives had been in danger, or who may have later been murdered. The categories provide insight into the weird world of these reports: believed drowned, possible suicides, elderly who wanders, possible domestic violence/foul play, signs of a struggle or blood/foul play, informant reported murdered/foul play. A key category for us was serial killer victim. This involved a lot of investigation into alleged serial killers who had operated in Washington since the 1980s. In the end, we convinced key detectives across the state to identify missing persons whom they suspected to be the victims of serial killers, based on evidence generated in investigations.

In addition to the missing-persons database, I built a database of unidentified dead persons - murder victims and others - by contacting coroners and medical examiners statewide. At that point we thought we knew who was missing and how many unidentified dead persons there were.

Next we compared the information more closely to official records of missing persons and unidentified dead persons. Surprisingly, the Washington State Patrol's missing-persons unit, which struggled to keep up with just one staff member and a volunteer dentist, quickly provided the data, though it withheld some key fields.

Official list
In each state a designated agency such as the Washington State Patrol collects information and feeds it into national databases of missing persons and unidentified corpses. The national databases are part of the huge and antiquated NCIC.

We received parts of the missing persons data and much of the unidentified persons data from the state. Journalists can obtain similar data from the authorities in their own states. In the missing-persons table, the State Patrol provided the name, agency taking the report, sex, age, report date, date of last contact, height, weight and eye color. The state also disclosed information about whether the investigating agency had provided information about dental records. Some departments ignored the law an important point in our series. We found that more than 60 percent of all missing persons had no dental reports on file.

The information we received in the unidentified corpses table was more limited and less reliable. The State Patrol provided the agency name, report date, height and weight estimates that were often inaccurate, age estimates, the condition of the body, cause and manner of death and availability of dental records.

Despite the limits of the data, I was able to analyze it and discover that some probable murder victims had never been entered as official missing-persons. That meant, when unidentified remains were found, the police lacked information for linking missing persons to the remains. I also was able to generate a statistical profile of the missing and play detective.

The NCIC tries to match the tables of the missing and the dead but, our reporting found, the automated search engine in the database was flawed. So I matched the tables we obtained to see whether I could find matches overlooked by the police. The match results generated new leads in several cases that the police are still investigating. After the series ran, readers provided tips that turned up other valuable leads.

So far, the identities of two long-unidentified bodies have been confirmed based on information generated by the series. I hope that more will be added to that list over time.

To see the series and a portion of the databases we used, see http://seattlepi.nwsource.com/missing.

Contact Lise Olsen by e-mail at LiseOlsen@seattlepi.com.

Uplink
May June 2003
5

==End of OCR for page 5==

==Start of OCR for page 6==
Visit our Web site www.nicar.org

MAPPING IT OUT
The latest uses of mapping in news reporting.

Adding the 3rd dimension
By Stephen K. Doig
Arizona State University

A few months ago, I was wowed by a demonstration of a mapping tool that was new to me. It was 3D Analyst, an extension to ESRI's popular ArcView geographical information system (GIS) software.

3D Analyst can take a two-dimensional map, such as a thematic map of census tracts, and transform it into a three-dimensional image in which the height of each area on the map can represent another variable. The resulting depiction of data, which looks like a cityscape of skyscrapers, can then be "explored" by turning it every which way, zooming in and out and even flying through it up close.

I decided I had to learn how to use this cool toy. Like most journalists interested in computer-assisted reporting, I learn new tools best when I have some real journalistic task to accomplish. The trick was to find something interesting to do.

I knew that an extension like 3D Analyst adds substantial new capabilities to the base GIS software, but also substantial new costs. 3D Analyst lists for $2,500, out of reach for all but the most well-heeled newsrooms - or for the lucky people like me who can afford to pay lower academic licensing costs. So I posted a note on NICAR-L listserv offering a free 3D map to the first newsroom that offered me a project with which I could learn the basics of this tool. Full trial versions of the extension for ArcView versions 3.x and 8.x are available on CD from ESRI.

Janet Roberts of the St. Paul Pioneer-Press quickly took up my offer. Having seen a nice 3D map of median income change data created by the Chicago Tribune (see http://images.chicagotribune.com/media/graphic/2002-08/4286800.gif), she wanted to do something similar for the city of St. Paul, looking at percentage change in median household income since 1989. However, she needed it in less than a week, and I fretted it would take me longer than that to figure out the software. (I do have a full-time job, you know.) But I gulped and said, "Sure, I can do it."

Turns out I needn't have worried. If you already know how to use ArcView, doing the basic work with 3D Analyst is a snap. So here's a handy step-by-step for ArcView 8.x in case you get your hands on the extension:

• The first step is to activate the 3D Analyst extension. Assuming you have installed and registered 3D Analyst, just launch the ArcMap application, then choose Tools\Extensions in the menu and click on the box next to 3D Analyst.

• Next, prepare the shapefile (a map file with associated data) that you want to turn into a 3D image. You can do this in ArcView, just as you already know how to do. Janet sent me a shapefile of St. Paul Census tracts already loaded with an attribute table that included for each tract the median household income for 1989

[Image of ArcMap with a thematic map of census tracts.]
Untitled ArcMap ArcView
File Edit View Insert Selection Tools Window Help
Layers
sotr bhinicome
PCTCHG
63.00001 - 110.0000
33.00001 - 63.00000
16.00001 - 33.00000
4.00001 - 16.00000
-12.00000 - 4.00000
Display Source
492094.03 4977517.54 Unknow

6
May • June 2003

==End of OCR for page 6==

==Start of OCR for page 7==
[Image of ArcScene showing a 3D map of census tracts.]
Untitled ArcScene ArcView
File Edit Wow Selection Tools Window Help
30 Analyst
Scene layers
sp_tr_hhincome
PCTCHG
63.00001-110.0
33.00001-63.00
16.00001-33.00
4.00001-16.000
-12.00000-4.00
Display Source

and 1999, the dollar change and the percentage change.

• To make a 3D image, launch ArcScene, which is 3D Analyst's workspace. You'll see that it looks a lot like ArcView, except with a few different buttons. Make sure you have the 3D Analyst toolbar turned on.

• Hit the Add Data button and find your shapefile. You'll see the expected map in one color, but at an odd angle. That's because the map already is in 3D. Click on the Navigate tool (the one that looks like a compass with four points) and then use the mouse to move the map around a bit, just to see how it works.

• Now right-click on the layer name and open the Properties window. You'll see the usual tabs, including Symbology where you can choose "Quantities... Graduated Color" to make the usual thematic map. Go ahead, make that map. So far, nothing much new.

• But notice that the Properties window has a few unfamiliar tabs, including Extrusion. That's the one that's going to give you the 3D effect. Click on the "Extrude features in a layer" box, and then the little calculator-like icon to the right of the expression box.

• In the Expression Builder window that pops up, click on a field you want to have provide the height (or z-value) variable. It can be anything numeric - altitude, population, density, income, percent, whatever. Tell it OK, then Apply.

• Here's the tricky part. Chances are, the variable's value by itself won't do what you want. If the values are small, it may not show much height at all; if they're large, the height may shoot off the screen. In either case, go back to the expression builder window and multiply the variable (if it was too small) or divide (if too high) by some arbitrary number, let's say 100. Your expression then might look something like [PCTCHG]*100. Hit Apply.

• If the new expression works, great. If still not, then pick another value in the right direction and try again until it starts to look like you want it to.

• Now explore the image using the Navigate and other pointers on the ArcScene toolbar. Note that you can twist and turn the image and even look underneath, so that it seems to be like Laputa, the flying island of "Gulliver's Travels." When you look underneath, you'll see that any areas with negative values are extruded downward. Move it around until you have an angle that illustrates what you are trying to show.

• Finally, go to File\Export Scene 2D on the menu to save the image in whatever graphics format you want.

3D Analyst will do much more than simply making a 3D version of a flat thematic map. But I'll learn more of those capabilities when someone comes to me with another project to try. Maybe it'll be you?

Contact Steve Doig by e-mail at steve.doig@asu.edu.

Would you be willing to share
a mapping example with
fellow journalists? Send an
electronic copy of the map
along with details to David
Herzog at dherzog@nicar.org

Uplink
May June 2003
7

==End of OCR for page 7==

==Start of OCR for page 8==
Mall our Wibsite www.ire.org

SPOTLIGHT: RACE
Ticket data shows search rate gaps
By Karisa King, San Antonio Express-News

The story began with a Texas law that went into effect in January 2002 and required every law enforcement agency in the state to track the race of drivers stopped and searched.

The law was a response to the national debate over racial profiling – the practice of police targeting minorities because of their race or ethnicity. Members of minority groups say the practice is widespread and that the police target people for "driving while black" or "driving while brown." But police officers and administrators insist that no such practice exists. For every claim of racial profiling they offer a plausible reason for the stop.

Seeking to go beyond this debate, the San Antonio Express-News filed a state open records request and obtained the first full year's worth of data for the city. The San Antonio Police Department provided the complete database free of charge. Using Microsoft Access I analyzed nearly 290,000 records detailing all the police traffic and pedestrian stops for 2002.

First venture
This was my first computer-assisted reporting story, done shortly after I attended a NICAR Boot Camp in March. The Boot Camp provided me with the basic tools I needed to make sense of the numbers. I relied mostly on simple structured query language (SQL) queries and used a few Microsoft Excel spreadsheets to determine the rate of police stops and searches for minorities and whites. The data was detailed enough that I could examine the type of searches police conducted, the reason for the initial stop and the outcome of every stop, including whether police found contraband or issued a traffic citation or a warning.

Before we could put the findings into perspective we had to leap one of our biggest hurdles: finding a point of comparison. Profiling experts agree that no convenient standard exists for comparing data. Census figures reflect a community's population, not the demographics of drivers. The data does not, for example, reflect the number of tourists or commuters on the roads.

Finding a measure
Express-News researcher Kelly Guckian helped me explore other benchmarks. Our options included Census data for resident populations, Census vehicle availability data, licensed driver population data or road surveys.

Profiling experts agree that no convenient standard exists for comparing data.

Hiring an outside consultant to perform a road survey would have been too expensive. We discovered that the Texas Department of Public Safety, which issues state licenses, records the race of drivers. But because the state lumps whites and Hispanics together in the same category, the DPS data was meaningless for us. And while the American Civil Liberties Union recommends using Census vehicle availability data we decided not to because those numbers reflect the number of vehicles available to a household, not the number of drivers.

In the end, we settled on using Census data for San Antonio residents but told readers up front about the debate over comparing the numbers and emphasized that our benchmark was imperfect.

We determined the stop rates by summing the total number of drivers stopped for traffic reasons for each race and calculated that as a percent of the total number of drivers stopped. Then we compared those percentages to the Census population percentages. To be thorough we also compared the numbers to the vehicle availability data and found that did not greatly alter the results. We did not publish this second set of results because we thought it would confuse readers.

To determine the search rates we compared the total number of drivers of each race who were searched to the total number of drivers of each race stopped for traffic reasons.

It took about three weeks to complete all the queries and compare the results. Among the findings:

Police stop minorities and whites for traffic reasons at rates that are roughly proportional to their share of the population. But during probable cause and consensual searches - instances in which officers use discretion - blacks are more than three times as likely as whites to face a search during traffic and pedestrian stops.

Although blacks are more likely to face consensual searches, police found contraband some type of evidence of a crime, stolen property or drugs at about the same rates for all three groups. During traffic stops that led to probable cause searches, whites were slightly more likely than blacks or Hispanics to have contraband. And during all traffic stops police found contraband on whites more often than blacks.

Reporting time
With those findings in hand it was time for the traditional reporting. I interviewed police officers and administrators, civil rights advocates, profiling experts and drivers. It was crucial to

8
May • June 2003

==End of OCR for page 8==

==Start of OCR for page 9==
follow up with drivers who had been stopped and, in some instances searched, to learn their perspectives. To find the drivers, I pulled the ticket numbers from the data and submitted an open records request to the San Antonio Municipal Court for copies of about 40 traffic tickets. The tickets, which listed the names and telephone numbers of the drivers, led me to several people who felt they had been victims of racial profiling. These interviews also provided vital contact with minorities who felt strongly that racial profiling is pervasive, yet failed to lodge their complaints with local civil rights groups or police because they feared retaliation or lacked faith in the system.

Some advice for other journalists planning a similar project: Request the data in advance. We wanted a full year of data but requested it about six months into the year in case we'd have to fight for access. In this case, however, the department did not put up a fight over releasing the data.

If the data includes information on searches, make sure you understand the kinds of searches recorded. Certain types of searches, such as probable cause and consensual, reflect officers' discretion. But other searches, such as those done during arrest or before police impound a vehicle, do not necessarily involve police discretion. If we had lacked the ability to filter out inventory searches and arrest searches, the data would have been useless.

In response to the article, San Antonio police acknowledged that the difference between the search rates of whites and blacks caught their attention. Before the story ran, the department had already planned to hire an outside consultant to produce a detailed analysis of the data by comparing it to an independently generated traffic survey. We plan to follow the results of that study when it is released in a few months.

Contact Karisa King by e-mail at kking@express-news.net.

Uplink

Unstacking the Deck: A Reporter's Guide To Campaign Finance

The latest Beat Book from Investigative Reporters and Editors, Inc., is a guide to navigating the language and practices of campaign finance.

Journalists will find it invaluable for pursuing stories about the impact of money on elections, political parties and candidates at the federal, state and local levels.

[Image of the book cover for "Unstacking the Deck"]
UNSTACKING THE DECK
A Reporter's Guide to Campaign Finance
BY MICHAEL A WEBER ARON PILHOFER AND DEREK WILLIS

• Understand the loopholes in soft money restrictions.
• Learn about the increased use of nonprofits to funnel money.
• Find out how to track where candidates spend their money.
• Learn how to find and read useful documents and data.
• Packed with resources, Web sites and story tips!

ORDER NOW

By phone: Call 573-882-3364 with your VISA or MasterCard.
By mail: Send your check to IRE, 138 Neff Annex, Missouri School of Journalism, Columbia, MO 65211
By Web: Visit www.ire.org/store to order online or download an order form.

IRE Members: $15 each • Nonmembers: $25 each

Plus postage: First class — $4 for the first book, $2 for each additional book

May • June 2003
9

==End of OCR for page 9==

==Start of OCR for page 10==
iffe Detaliate Library 573.884.7711

SPOTLIGHT: RACE
Toronto profiling stories spark a firestorm
By Stephanie Kang, IRE and NICAR

When Toronto Star reporter-photographer Jim Rankin submitted a Freedom of Information request for a database detailing arrests in Toronto, he knew the police wouldn't just hand over the data. There was no precedent for such an expansive request to a Canadian law enforcement agency and no database editor at the Star to help him.

Although the Star had already lost plenty of FOI battles to the Toronto police, Rankin felt that the paper's commitment to his request was strong enough to prevail this time. He had hope.

That hope was rewarded April 10 when Rankin and four other Star reporters received the Michener Award for meritorious public service journalism for their series, "Race and Crime." This year marked the first time that the award, one of the most prestigious awarded to Canadian media, recognized a computer-assisted reporting project done on such a large scale. (Read the series online at www.torontostar.ca/NASApp/cs/ContentServer?pagename=thestar/Render&c=Page&cid=1034935301156.)

On the surface, the Star series told a familiar story: "Blacks arrested by Toronto police are treated more harshly than whites" in simple drug possession cases and non-traffic violations. What was different was how Rankin and the Star team reported the story, by analyzing one of the largest police databases ever to be publicly released in Canada.

The Criminal Information Processing System (CIPS) database lists arrests from 1996 to early 2002. The data includes more than 480,000 incidents in which an individual was arrested or ticketed, and nearly 800,000 criminal and other charges, such as traffic offenses and city ordinance violations. Rankin used Access to count and group the data by race, employment, immigration status, simple drug possession charges and tickets for non-traffic violations. Rankin found that the police took blacks charged with simple drug possession to a police station more often than whites facing the same charge and that the police held black suspects overnight for a bail hearing at twice the rate of whites. In cases of non-traffic violations, the analysis showed that black motorists were ticketed disproportionately compared to whites.

Reaction
The series, which ran in October and November, stirred many Toronto residents who have long believed that the police practiced racial profiling. Government authorities reacted quickly, opening an inquiry into racial profiling and meeting with community and police leaders. Toronto Police Chief Julian Fantino announced a race relations outreach program.

Critics of the series have complained loudly. The Toronto Police Association, after alleging that the Star series had damaged the reputation of the force and its relationship with civilians, lodged a $2.7 billion lawsuit set for trial in June. Analysts hired by Chief Fantino performed an independent analysis of the Star's findings, calling it "junk science."

Debate on the "Race and Crime" series has spread elsewhere in the country, spotlighting not only the possibilities of CAR, but also laying bare some of the obstacles Canadian journalists face for access to information.

The Star reporters had heard allegations of racial discrimination by the Toronto police, but without numerical data to support the claims, all they had were anecdotes. Although a 1999 Canadian policy prohibited police from analyzing their database from the standpoint of race, the Star was under no such restriction.

Requesting electronic data from a government database in Canada, however, presents a combination of problems familiar to U.S. journalists. Alasdair Roberts, an associate professor at Syracuse University who studies Canada's FOI laws, said Canadian journalists face two main obstacles in obtaining government information.

One is the government's desire to sell access to information. "The argument is that releasing the information in bulk form would cause damage to themselves," Roberts said. In the Star's case, the police initially argued that commercial factors barred them from selling the data to the paper because the agency had already sold the database software to an Australian police force. It took Rankin months to convince the police that he wanted the data, not the database program.

"In their defense, they didn't understand what language we were speaking," Rankin said. "But there were a couple of times when I felt they weren't acting in the spirit of the law." In one instance the police refused to release a list of fields, in another the police sought to charge $7,000 (more than $4,900 in U.S. dollars) to produce a three-page list of fields that a FOI commissioner determined should cost 60 cents (42 cents in the United States). In total, the Star spent $800 (more than $560 in U.S. dollars), more than two years in negotiations with the Toronto police and six weeks reporting the story.

The other obstacle is Canada's strong personal privacy protections. The Access to Information Act has not been overhauled since its adoption 20 years ago despite promises made by a justice minister in 1994 to review the law and other reform attempts.

10
May • June 2003

==End of OCR for page 10==

==Start of OCR for page 11==
To obtain CIPS, the Star made concessions to data that the police believed would protect privacy: Instead of getting exact dates of arrest, the paper accepted approximations. Instead of getting the names of the people arrested, it accepted a randomly generated identifier.

The biggest alteration, however, involved the criminal charge field. The police said that the rarity of some charges, along with other personal identifiers, could allow the Star to visit one of Toronto's six courts and identify a suspect. So Rankin recoded the Criminal Code of Canada and the Ontario Highway Traffic Act, crime-by-crime, into nondescript categories. For instance, where the original database listed individual charges such as manslaughter, murder or sexual assault, the Star's database recoded these charges into one field called "Violent major."

Finding focus
Rankin narrowed his analysis to cases where police could use discretion, like drug possession charges and non-traffic violations. When charged with drug possession, an officer may take one of three actions: ticket the offender, arrest the offender or arrest the offender and hold that person overnight for a bail hearing. After Rankin found that the police released blacks less often than whites, he dug deeper into the data to check whether immigration status, employment or the presence of a home address made a difference. The numbers changed, but the difference persisted, Rankin said. Analysis on non-traffic violations showed disproportionate numbers of blacks ticketed for offenses that could not initially be known by an officer, like an expired license, or a warrant for arrest.

The Star hired York University professor Michael Friendly to examine Rankin's findings. Friendly used S-Plus and R, two freely available statistical packages, to examine the data and test the potential effects of factors such as criminal history, the existence of a home address and employment.

Soon after the newspaper published its stories, Chief Fantino hired Edward Harvey, a professor at the University of Toronto, to examine the Star's work. In his report (available on the Toronto police department's Web site at www.torontopolice.on.ca) Harvey concluded the Star failed to adequately clean the data, used inconsistent methods and failed to provide evidence of systemic racial profiling. The Star published a rebuttal of Harvey's criticisms, and as Rankin said, the two parties continue to "butt heads, while talking about the underlying issues is left behind."

CAR boost?
Friendly sees the stories as not only as a "fundamental breakthrough in Canada" but also as part of a new movement in the country in which journalists present complex social situations to the public by means of analyzing real data, not just anecdotal evidence.

"CAR here is still in its infancy," Rankin said. As a result, government agencies lack respect for CAR, and Canadian media organizations have lagged their counterparts in the United States.

One of the biggest obstacles to CAR in Canada is a lack of support from management, said Robin Rowland, of CBC News. "Most CAR people [in Canada] are still crying in the wilderness. Most of the managers in Canadian news media don't see value and don't see money in it."

The stories' potential effect on freedom of information in Canada seems dim, according to some.

"I would hope stories like the Star did would change access to information, but I wouldn't bet my last shirt on that," said Pierre Bergeron, president of the Michener Award Foundation.

So what will it take to move CAR in Canada from infancy to adulthood?

"We need an updated act, which probably won't happen until we have a new prime minister," Rowland said. "We need a change in attitude in the department of civil service. And we need a realization from news management that CAR is a valuable, effective tool for journalism."

Rankin sees another way, one that worked for his newspaper and doesn't involve spending money, lobbying for a change in freedom of information laws or seeking changed attitudes among news managers: persistence.

"I hope that every little paper across Canada is asking for the same data [we did], because it should be there for the asking," Rankin said. "Being persistent works and it's free."

Contact Stephanie Kang by e-mail at stephanie@nicar.org.

readme.txt
The Seattle Times examined more than one million traffic stops and found black motorists in some areas of the state were more likely to be searched than whites. The story is available on the Web at http://seattletimes.nwsource.com/html/localnews/134609738_racial05m.html.

The Boston Globe analyzed more than 750,000 traffic tickets from police departments in Massachusetts and discovered racial disparities in many areas of traffic enforcement. The two-part series is available at www.boston.com/globe/metro/packages/traffic/010603.shtml.

The Providence Journal also examined ticket data and found that some Rhode Island police departments searched vehicles driven by blacks and Hispanics as much as five times as often as those driven by whites. Read the story on the newspaper's Web site, www.projo.com. Registration is required.

Uplink
May June 2003
11

==End of OCR for page 11==

==Start of OCR for page 12==
Visit our Web site www.nicar.org

Segregate
continued from page 1

No matter what their beat, most journalists will end up writing about some kind of segregation: racial, economic or other. Do minority neighborhoods have more crime? Do children of different races learn in the same classroom? Where are the poorest and richest neighborhoods in a city? How did the racial makeup of cities, counties and Census tracts change from 1990 to 2000?

Controversy
Just as race is a sensitive topic, the measures of segregation used or reported by journalists can generate controversy. A study by researchers at the University of Wisconsin-Milwaukee challenged Milwaukee's reputation as one of the country's most segregated areas. The researchers looked at the percentage of residents living in blocks that were at least 20 percent black and 20 percent white in 2000. Milwaukee ranked as the 10th most integrated of the 50 largest metro areas. The metropolitan area ranked as the 43rd most integrated out of 100 - not great, but much better than its standing under traditional segregation measurements.

The Milwaukee Journal Sentinel reported the study's results in a three-day series called "Closer Together: A More Integrated Milwaukee.” (Read the stories at www.jsonline.com/news/metro/jan03/109872.asp) The university study and newspaper series prompted a flurry of letters, phone calls and discussion, in Milwaukee and beyond (including the CENSUS-L listserv run by IRE and NICAR). Some researchers criticized the study for its methods (for instance, some said the 20 percent figure in the Milwaukee report was arbitrarily chosen) and the newspaper for publicizing the findings without questioning them. The newspaper and the researchers, at UWM's Employment and Training Institute, have publicly stood by their work.

The lesson of the controversy is that even the most math-phobic journalists should have a basic understanding of segregation and how it can be measured. According to one prominent sociologist, most journalists are doing a good job.

"Actually, I think it's outstanding that so many reporters have been able to use numbers, whether they use one number or several, and interpret them correctly," said John Logan, a professor at the State University of New York at Albany. Logan talks frequently to reporters as director of the Lewis Mumford Center for Comparative Urban and Regional Research.

Logan said he's seen more sophisticated analysis of segregation with Census data in 2000 than in 1990. "I guess editors have been willing to allow more real information in the stories and to rely less on anecdotes."

Some stories
Some of those segregation stories appear on the "In the News" section of the center's Web site. (www.albany.edu/mumford) Last year, for instance, The Detroit News published a series called "The Cost of Segregation" that examined attitudes about race and the social, economic and political consequences of segregation. Working with 2000 Census data, The Detroit News used several statistical tools to measure segregation.

In Cleveland, The Plain Dealer analyzed economic isolation using 2000 Census data. Reporters looked at the number of people who live in Census tracts that are either totally wealthy (median household incomes greater than $75,000) or totally poor (at least half of households below the federal poverty level). They found that about one in five residents in the metro Cleveland area lives in an economically segregated neighborhood.

One of the challenges, said Dave Davis, the reporter who leads the paper's Census coverage, is sifting through all of the reports and statistics on segregation and explaining them in a way that makes sense to readers.

Options
Sociologists and statisticians have found at least 20 ways to measure segregation. In an influential 1988 study, two researchers grouped those measures into five main categories or "dimensions" of residential segregation:

• Evenness - whether minorities are over-represented in some areas and under-represented in others, or spread out evenly.

• Exposure - the potential for members of different racial groups to interact with each other by virtue of living in the same neighborhood.

• Concentration - how much physical space members of a minority group occupy compared to members of a majority group.

• Centralization - whether members of a minority group live close to the urban center or out in the suburbs.

• Clustering - whether minorities live in adjoining neighborhoods or in neighborhoods scattered throughout a metropolitan area.

Journalists and researchers tend to measure evenness the most. A popular measure of evenness is the index of dissimilarity, which compares the racial makeup of individual neighborhoods to that of the entire metro area. The index ranges from 0 (total integration) to 1 (total segregation). A dissimilarity score of 0.85 for blacks means that 85 percent of blacks would have to move to another neighborhood to attain perfect integration. On the plus side, the dissimilarity index can be used in other ways, such as comparing the racial composition of individual schools within a school district. On the minus side, only two groups can be compared at a time.

Another measure of evenness, the

12
May • June 2003

==End of OCR for page 12==

==Start of OCR for page 13==
Gini coefficient, is often used to measure how equally income is dispersed in a society. A coefficient of 0 represents perfect income equality, and a 1 means perfect income inequality.

A common measure of exposure is the isolation index. It gives the probability that a person shares an area (such as a neighborhood or Census tract) with another person of the same race or ethnicity. Like the dissimilarity index, it ranges from 0 to 1, with higher values meaning more segregation. An isolation index of 0.85 for whites would mean that the typical white person lives in an area that is 85 percent white.

In a report released in August, the Census Bureau ranked 43 metropolitan areas by each of these measures. (The report, "Racial and Ethnic Residential Segregation in the United States: 1980-2000," is online at www.census.gov/hhes/www/resseg.html)

The results show how complex measuring segregation can be. Detroit ranked as the most segregated city based on evenness and clustering. New York topped the list based on exposure, while Milwaukee was first based on concentration, and Minneapolis was first based on centralization.

The report also shows how a single metro area can appear more or less segregated, depending upon the definition. Atlanta ranked as the 11th most segregated area in exposure, 15th in clustering, 23rd in evenness, 35th in centralization and 42nd in concentration. The Riverside-San Bernardino area ranked anywhere from 3rd to 40th most segregated, depending on the measure.

So which measures should reporters use? Much of the answer depends on what the story is about or the demographics of a particular area.

For its recent series on economic and racial segregation, The Detroit News looked at several measures but focused on the dissimilarity index and isolation index. Brad Heath, one of the reporters who worked on the series, said the dissimilarity index "provided the most simple and direct measurement of the extent to which affluent blacks and whites were living in the same neighborhoods."

The paper also used the isolation index because it's easier for readers to understand, according to Heath: "Instead of explaining dissimilarity and how it works, we were able to show them how 'typical' rich whites and blacks live in very different neighborhoods."

The Plain Dealer also has used different gauges. For instance, the paper measured racial isolation by counting Census tracts where nine of 10 residents are of the same race. They've also used the dissimilarity index and the USA Today Diversity Index, which measures the probability that two people picked at random will be of a different race.

In Wilmington, Del., The News Journal used both the dissimilarity index and the diversity index to measure racial integration. First, the paper eliminated areas that were too racially homogeneous to have segregation. To do that, reporters calculated the diversity index for areas with an index of 0.25 or higher (meaning there is a 25 percent or greater chance that two people picked at random will be of a different race). The paper did so because the dissimilarity index is sensitive to small numbers, according to Aron Pilhofer, who co-wrote the story. (In other words, if a city has 1,000 residents but only 30 of them are minority, they would have to be individually scattered for neighborhoods to reflect the overall racial makeup of the city.) For the remaining blocks, the paper calculated the dissimilarity index then hunted down the most integrated, and segregated, areas of the state.

Some advice
• Think visually. Segregation deals with the physical space people inhabit so it makes sense to use maps to supplement your analysis and charts. The Detroit News used a dot-density map to show where the wealthiest whites and blacks lived in 2000 (See map at www.detnews.com/pix/2002/11/03/segregationmap.gif). Ninety-nine percent of wealthy whites lived in the suburbs, while 61 percent of blacks lived in the city of Detroit. "Putting a precise definition on segregation's never easy, but the maps make undeniable the claim that whites and blacks are stratified in ways that have little to do with money," reporter Heath said.

• Ask experts for help. Many reporters check with university researchers when deciding how to measure segregation. They can help you choose a measure and review your results.

• Read up on the issue. Paul Overberg, a Census expert with USA Today, recommends two books: "American Apartheid: Segregation & the Making of the Underclass,” one of the definitive books on segregation, by Douglas Massey and Nancy Denton; and "The Next American Nation: The New Nationalism and the Fourth American Republic," by Michael Lind.

• Don't ignore common sense. Does the real world reflect what the statistics show?

• Remember that some things can't be measured. Even if a neighborhood is perfectly integrated, people of different races may still eat at different restaurants, have different friends or attend different churches.

Overall, journalists should resist the urge in their profession to boil complex issues of race down to the point of losing important details and nuances. As Logan said, "There's no single number that would tell you what the story is."

Contact Holly Hacker by e-mail at holly@nicar.org.

Uplink
May • June 2003
13

==End of OCR for page 13==

==Start of OCR for page 14==
JRE Database Library 573.884.7711

CAR TOOL
OS X lays foundation for CAR on the Mac
By Derek Willis, Center for Public Integrity

Last summer I did something I never thought I would do: I bought a Mac. I wasn't a "Mac guy." In fact, I had always considered Apple computers novelties that were nice to look at and easy to use, but not for me. All of my favorite software ran on the PC and little of it ran on the Мас.

Then came Apple's new operating system, OS X (called OS Ten) and those gorgeous PowerBooks that ran it. I wanted one, not just because it was a lot prettier than my dull laptop. After reading reviews and trading e-mail with PowerBook users, I decided that I was ready to make the switch.

So I unloaded my PC laptop and got a used PowerBook that handles serious computer-assisted reporting work. From spreadsheets to databases, scripting to Internet utilities, OS X has plenty of options available for journalists doing CAR.

[Image of a computer screen showing a spreadsheet in OpenOffice.org]

Even if you work in a Mac-based newsroom you may not have seen OS X. It's still new and many older Mac applications haven't been updated to run on OS X (Most will function, but often sluggishly). Unlike many new versions of Windows, OS X isn't merely an upgrade. It's an entirely new operating system based on Unix. That means OS X is not only sturdy and reliable, but it can run scores of applications that are designed for Unix and its open-source cousin, Linux.

OS X is extremely stable. I can count on two fingers the number of times I've had to reboot my machine since last summer, and both of those occurrences can be blamed squarely on user error. In fact, if a program stops responding you can shut it down without affecting the rest of the system at all.

The desktop looks a little like the older Mac versions, but with some great new features. The first is the dock, which serves as a placeholder for favorite applications. It's also where OS X puts minimized programs. Using the familiar bar atop the desktop you can connect to remote servers, search for files and log onto the Internet.

OS X's search function alone is worth mention. Besides being easy to use, it also has advanced features that permit you to index the contents of folders holding text files, Web pages and other common documents so that you can search them. The indexing feature can be turned on or off for any folder, meaning you won't fill up the hard drive with unused indexes. The rest of the desktop can be as cluttered or uncluttered with program icons and folders as you like, but there is one very un-Mac object: the terminal window.

CAR gateway
This is the gateway to OS X's Unix-based system and a primary reason why this OS is ready for CAR. The terminal window can be used to launch Unix applications, such as the MySQL database, to run Perl scripts or telnet into remote computers. It's much like the DOS command line. The terminal program provides access to the Unix file system and the Mac files and folders.

When it comes to the critical CAR programs - spreadsheets and database managers - OS X has some good choices available but other shortcomings. First the spreadsheet: Microsoft makes a version of Office for OS X that includes Excel. If you do not want to pay several hundred dollars for it you can opt for the spreadsheet that comes with the free OpenOffice suite for OS X. (Read "Office package needs more work" in the January-February 2003 Uplink for details.) The spreadsheet, called Calc, reads and writes Excel files and generally is a decent substitute.

Many of the leading PC database managers, such as Microsoft Access

14
May • June 2003

==End of OCR for page 14==

==Start of OCR for page 15==
[Image of a computer screen showing the "Jefferson" application for OS X]
Jefferson File Edit Window Help
Search for bills
sponsored by
Representative Wilson
Bill Subject Sponsor
10th Congress
Fetch

and FoxPro, won't run on OS X without emulation software, which can be fussy, but that doesn't mean users are out of luck. OpenOffice users can use an Access-like query by example grid to query databases via ODBC (Open Data Base Connectivity) but will first need to set up the ODBC connection. An open source developer is working on a Unix-based set of tools that can read Access .mdb files. For the next version, the developer plans to add file-writing capabilities.

Data work
One of the more popular database managers for OS X is MySQL, a powerful program that will be familiar to most FoxPro and SQL users, and pretty easy to pick up for others. (See "MySQL primed for heavy-duty action" in the March-April 2003 Uplink for a review of the program). Many of MySQL's powerful features can be accessed using the terminal's command line, but OS X developers have created some nice front-ends to make the database work even easier.

My favorite is CocoaMySQL, a freeware program that allows users to write and save SQL queries and has an Access-like filter function. Its tabbed interface makes it easy to switch back and forth between tasks, such as adding a field to a table and running queries. The program is still being refined and more features are in the works, according to Lorenz Textor, its developer. (Why CocoaMySQL? One of the two main programming languages for OS X applications is Cocoa. The other is called Carbon.)

Another option is to administer MySQL through a Web browser using PHPMyAdmin, another open source program that requires the Apache Web server and some fine-tuning to set up. See www.devshed.com/Server_Side/Administration/BuildingOnOSX/page1.html for details.

Importing data into MySQL is even easier on the Mac than a PC because while the best import utility runs via the command line, OS X allows you to drag and drop file names and locations between folders and the terminal, saving plenty of keystrokes. And MySQL's biggest advantage over other database managers is its speed.

Mapping fans don't have much reason to switch to OS X yet, but an open-source GIS program called GRASS runs on the Mac. Still, it's hardly a substitute for more refined programs, such as ArcView, MapInfo and Maptitude.

Utilities
What about the other little components in the CAR toolbox? OS X runs many great basic utilities, including the FTP program Transmit ($39) or the free RBrowserLite, both of them similar to popular Windows FTP clients. The all-purpose text editor BBEditLite, which is free, or its more powerful full version ($149) have extensive search and replace functions and can do some things Word doesn't, such as save a file directly to an FTP site.

Initially, Web browsers for OS X disappointed me. Microsoft Internet Explorer for Mac (5.2) isn't nearly as good as the PC version, but Apple recently introduced a new browser, Safari, that works quite well. You can also install other popular browsers such as Netscape, Mozilla or Opera.

There are some things I do miss about Windows: the right-click, for instance. But the functionality isn't missing in OS X, it merely requires a switch to a keyboard combination. And if you can't completely give up your Windows PC, OS X easily reads Windows disks and e-mail attachments, making file transfers a snap. I even connected my PowerBook to my former company's internal network and was able to access network folders with no trouble.

Maybe the biggest upside of OS X is that it has a vibrant developer community that already has produced a number of useful applications. They range from Jefferson, a front-end to the Library of Congress' popular Thomas Web site, to NetNewsWire, which reads "feeds" produced by news sites and weblogs. Each week brings a new application or tool to try, and another reason why OS X has given Apple much more than a pretty face.

Contact Derek Willis by e-mail at derek@thescoop.org.

Uplink
May • June 2003
15

==End of OCR for page 15==

==Start of OCR for page 16==
ween our Web anewww.nicar.org

OPEN SOURCE
Putting a useful face on MySQL
By Aron Pilhofer, Center for Public Integrity

MySQL is a free, high-end database server with speed, features and raw power that compare favorably to applications that cost thousands of dollars. (See the March-April 2003 Uplink). But users may be surprised to find that the program does not include a client application for querying and managing data.

The good news is there are plenty of clients developed by others. With MySQL's popularity growing during the past decade, dozens of third-party clients - freeware, shareware, open source and commercial – have been developed.

The bad news is that I have not found one that combines rich features, stability and ease of use in a single package, though some come close. All of the popular clients discussed below, except MySQL Front, are in heavy development, so a year from now it is likely a do-it-all tool will emerge.

MySQL-Front
Freeware, Windows Only
http://efux.de/mysqlfront/MySQL-Front_2.5_Setup.exe

Pros: MySQL-Front may not have the most features, but it has the most useful ones and is well suited for serious data analysis. Its layout is nearly identical to Microsoft SQL Server's Query Analyzer. MySQL-Front's tabbed layout puts the query design window up front where it belongs and keeps the number of open windows to a minimum. The query results pane and the database management and design functions are a mouse click away under tabbed windows of their own.

Servers, databases, tables and field names are accessible from a menu tree along the left side of the client. Along the bottom is a scrolling log of commands passed to the database server - a useful feature. Many of my database tasks are repetitive updates of files collected over several years. It's sometimes difficult to keep track of which tables have been updated. The log solves that problem. Every time I perform a major update, I save a copy of the log. The log provides a record of the updates and allows me to reconstruct a table from scratch. To do that, I just run the log files (a string of SQL commands) on the raw data.

Cons: The biggest problem with MySQL-Front is that development halted with version 2.5, released in September 2002. MySQL-Front also lacks some of the more advanced import capabilities of other clients, such as the ability to import MS Access files to MySQL.

Bottom line: MySQL-Front is one of the two clients I have on my desktop all the time. It is one of the most mature MySQL front-ends available at any price.

[Image of the MySQL-Front software interface showing a database table.]

Mascon
Shareware ($49), Windows and Linux
www.scibit.com

Pros: Mascon is another rugged client application that includes most of the critical features users need. Mascon makes managing tables and indexes a snap. Creating a table is as easy as typing in a name for each field and defining the type of data to be stored there. Indexes can be found under a tab in the same dialog box.

Mascon excels at importing data. No other client imports data as seamlessly, especially from Microsoft Access, FoxPro or Excel. MySQL, like most database servers, is fairly strict about how field names and how certain data types - especially dates - are formatted. Access is a good deal more liberal about such things, and that's where problems arise when trying to transfer data back and forth. Mascon handles these potential stumbling blocks.

Cons: Mascon lacks some of the features that should be standard in any client, most notably the ability to manage

16
May • June 2003

==End of OCR for page 16==

==Start of OCR for page 17==
age user accounts and permissions. It also lacks the clean, convenient query interfaces of MySQL-Front and MySQL Control Center (see below). The query designer is a separate window from the list of tables, and there's no way to view the field names without opening the table in a new window. Unless you have memorized every field you want to query, your desktop is likely to end up awash in open table views and query windows.

The cost is a potential drawback, although the $49 license covers future upgrades.

Bottom line: Mascon is worth the price for the import and database design features alone.

SQLyog
Freeware, Windows
www.webyog.com

Pros: This is the one client that may push MySQL-Front off my desktop. SQLyog borrows heavily from MySQL-Front in terms of design, but also offers some nice improvements. SQLyog puts the query design window right up front. Underneath are tabbed windows showing query results, server messages, details about the active table and a log of commands.

A useful feature is the ability to apply a quick filter to results set based upon a value of that set, similar to an autofilter in Excel or Access. The program supports importing from text files or an ODBC data source. The ODBC import wizard worked well in limited testing and allows a great deal of control over importing. If a particular column name is troublesome, for example, the wizard allows users to assign a different one to the target table.

Cons: A few minor quibbles. The menus could be simplified, and there is far too much overlap between menus and icons. The layout is not intuitive, and is somewhat confusing. There is no way to browse your data. To scroll through a table, you have to execute a select all query. Perhaps the largest drawback with SQLyog is that it is freeware and not open source. Just as development on MySQL-Front stopped when the developer called it quits, the same could happen with SQLyog.

MySQL Control Center
Open Source (free), Windows, Linux (OS X soon)
www.mysql.com/downloads/mysqlcc.html

Pros: Control Center is the closest thing to an official client. It is an open source project of the company that develops MySQL, though it remains a separate product. As of this writing, Control Center was still in beta (version 0.9.2 is the most current), but it is already one of the better clients available.

I like the server/database/table tree along the left. But rather than list field names as a fourth branch of the tree, they are displayed in the center pane, which is a bit less convenient than MySQL-Front. The query design window, unfortunately, has to be opened separately from the main application.

Control Center has a command log and the program includes outstanding tools for server and user management. The menus are easy to find and very intuitive. Table design and indexing is also very simple and straightforward.

Cons: Control Center lacks quite a bit right now, including documentation of any kind. Most of the functions and features are easy to figure out if you're familiar with similar environments. But to those new to MySQL, this could be an issue. Control Center also does not have any data import or export features. There also are a few annoying quirks. For example, the results pane is limited to 1,000 records with no apparent way to alter it.

Bottom line: This could be an outstanding client down the road, but it's not there yet.

Other clients to consider:

DBManager Professional
(formerly DBTools)
www.dbtools.com.br/EN/index.php

This tool has every feature imaginable and more. It can import from just about any format, has an Access-like query designer and all the administration tools anyone could want. So what's not to like? Plenty. For starters, the import tool chokes when dealing with odd Access tables or field names, or large amounts of data. It also does not import Access 2000 tables. The table design window is clunky and complex. Worst of all, the program sucks up memory and eventually freezes if a query returns a large result, more than a few hundred-thousand records.

Navicat
www.mysqlstudio.com

Navicat is an outstanding commercial product, with some features I wish other clients supported. It is one of a few clients that sport a true Access-like query design grid. Users can create relationships between tables by dragging and dropping, which is a godsend when you need to join large tables and multiple fields. But the menus and general layout are cluttered and confusing. The biggest downside to Navicat is price: It costs $88 for a license, good for one year of support and upgrades.

These are just a sampling of the clients available for MySQL; there are many others, including the Access-like database query client built into the OpenOffice suite. Access itself can serve as an outstanding front-end for MySQL.

Contact Aron Pilhofer by e-mail at apilhofer@publicintegrity.org.

Uplink
May • June 2003
17

==End of OCR for page 17==

==Start of OCR for page 18==
lour Web www.ire.org

Tech tip...
Access forms ease data input drudgery
By Matt Scallan, Times-Picayune

Sometimes the biggest hurdle in computer-assisted reporting is getting the information into the computer.

We've all read great stories gleaned from forms, traffic tickets or other paper records that had to be punched into a database one field at a time.

It's a lousy job, but if you have to do it, Microsoft Access database manager has a lot of tools that can significantly speed up your work. The least glamorous job on my beat is typing police reports every other week. It's not fun, but using a customized Access 97 form with AutoCorrect functions, input masks and lookup tables accelerates the process.

I use separate Access forms to enter information about arrests and incidents that are then combined into a single report for use in our Harris NewsMaker pagination system. Access sorts the incidents and the people arrested, adds our newspaper's boilerplate language and formatting, then exports the data into a text report.

Using these tools, it takes about 10 keystrokes to type a report of a vehicle burglary in the 2200 block of Veterans Memorial Boulevard on Aug. 22. An added benefit is we store our police reports in a database, something that makes it easier to track and map crime trends. Our bureau has used the data to report stories about a rash of auto thefts outside movie theaters and holiday-season car burglaries.

These techniques can be applied to any data entry project.

Combo boxes
Our incidents form uses a combo box (drop-down list box) that allows us to pick from a table of street names and enter that data into a field in another table.

We got our streets table from the ESRI Web site at www.esri.com/data/download/census 2000_tigerline/index.html. After downloading the streets file for our area from the Web site, we imported the dBASE table that came in the package of files into Access. You may have to do some data cleanup in the streets table in Access because some of the entries typically have errors.

Working in the form design view, I placed a combo box onto my form. That launched a combo box Wiz-

[Image of a Microsoft Access form for data entry.]
IncidentFrm: Form
Date 104202/2003
Incidents
Community
Keyboard Shortcuts
Address Street
Add Street
Veterans Memorial Boulevard
Crime
Vernon Street
Versailles Street.
Veterans Memorial Boulevard
Veterans Memorial Service Road
Vic Street
Vic A Pitrie Drive
Victoria Avenue
Victoria Street
Date
Details
Add Record
Delete Record
Add Arrest
Switchboard
Help
View Past Incidents
Quit
Record: 1 of 3

18
May • June 2003

==End of OCR for page 18==

==Start of OCR for page 19==
Uplink

ard that guided me step-by-step through the process of linking the combo box to the streets table and instructing Access to place the selected street name into a field in the incidents table.

Typing in street names is time consuming, especially if you don't remember whether something is a street, drive, avenue or boulevard. The combo box speeds up the work by displaying matching street names when you type the beginning of the street name inside the box.

With the combo box in place I can type "Veter" and "Veterans Memorial Boulevard" pops into place. In addition to entering data with combo boxes like this, you can use a combo box to query a field.

Corrections
I used the Microsoft Office Name AutoCorrect feature (available through the Tools menu) to change abbreviations typed in the incident box of the form to the full phrase. For example, when I type "vburg," Access automatically converts it to "Vehicle Burglary." Just make sure you don't you pick an abbreviation that you would use in a news story.

I added buttons in design view, launched a Wizard and created command buttons to add records, delete records, and execute a query. When creating an "add record" button, I always label it Add & Record. The ampersand underlines the R and activates the Alt-R as the hot key.

I used input masks to save keystrokes when entering dates in both the incidents and arrests forms. (For more information see article Q311167 in the Microsoft Knowledge Base at http://support.microsoft.com). If all of your data comes from this year, the input mask 99/99/"2003";0;_ lets you complete the data by typing the four digits for the month and day. When Access sees a "9" in an input mask it requires you to enter a digit. A zero is an optional number. Use this input mask, "99/99/0099", for dates of birth. Why? Because Access treats 1/01/29 as Jan. 1, 2029, not 1929. If you must type in dates earlier than 1930, using double zeros gives you the option of typing in the four-digit year.

The output
I created an Access report to create a text file of the arrests that's ready to import into our pagination system. The report sorts arrests by community, street and the last name of those arrested. This is the code, inserted into the detail section of the report in design view, that does this:

=Trim([First] & " " & [Last] & "," & Age ([DOB]) & " " & [address] & ", " & [city] & ", was arrested " & Format([Date], "mmmm d") & " in the " & [block] & " " & [location] & ", " & [Community] & ", and booked with " & [charge] & " " & [Details] & " " & [Notes] & "É")

The code merges the fields together into a text file and separates them with spaces. The Age function in the code converts the information in the DOB field into a numerical age based on the date the report is run.

The specialized characters are unique to our computer system. I punched in the ASCII code for these characters to produce them since the generic version of MS Word handles them differently.

Contact Matt Scallan by e-mail at mscallan@cox.net.

readme.txt

Download a copy of the police reports and incidents Access 97 database from the NICAR Web site at www.nicar.org/techtips.htm.

For more information about using Microsoft Access forms to build database applications see "User Friendly Forms" in the April 2000 Uplink and "Fronts for Forms" in the June 2000 Uplink.

May • June 2003
19

==End of OCR for page 19==

==Start of OCR for page 20==
IRE Darturbass Ultrary 573.884.7711

Priests
continued from page 1

porters' work. (For more details about The Globe's investigation see the May-June issue of The IRE Journal).

"The database was very helpful in the beginning. It basically showed us there was a big problem and gave us a lot of confidence," said Matt Carroll, a reporter on the team. Initial reporting gave the team an idea that there was a larger problem within the Boston Archdiocese; what they needed was further evidence of this to move on with the project.

The process that followed was long and tedious. The resulting database, though, produced the substance the team needed.

The Globe reporters learned that the archdiocese had placed some priests accused of inappropriate sexual conduct on leave for a number of years. Following this lead, Spotlight editor Walter V. Robinson scanned through each year of the archdiocese's annual directories and placed a checkmark next to each priest listed as being on leave. After identifying these priests, Carroll and three other reporters settled around a computer, took turns reading the information and typed data into Microsoft Excel.

They then created a spreadsheet for each priest using the information found in the annual indexes. Each spreadsheet included the priest's name, the year of assignment and where or how they were assigned. When the reporters were finished each priest in the database had 16 records, one for each year of assignment. Later, records of civil litigation were added. While The Globe has used databases fairly often for projects this was the first time they created a year-by-year database for analysis, Carroll said.

Probe for patterns
Combining the data with the knowledge that certain on-leave patterns possibly indicated priests with recurring sexual misconduct, Carroll was able to search for trends within the data. While taking into consideration that some of the priests were ill and legitimately on leave, the patterns that developed showed Carroll that some priests had repeatedly been placed on sick leave or went unassigned for a number of years. These findings corresponded with what the reporters had been discovering and was later supported through their reporting.

Court documents and data on the intranet helped reporters verify information and conduct background checks of the priests. The database allowed reporters to provide concrete evidence of the phenomenon that had before only been hearsay. It also provided them with the numbers necessary to support their reporting.

"It showed us that the number of priests on sick leave tripled in the 1990s," Carroll said. "The initial database stuff showed us there was a problem that gave us the confidence to push on hard with it."

Despite this, Carroll said what carried the story was not the database, but the documents they acquired and the diligent reporting that put it all together.

With the Boston scandal getting national attention and an approaching meeting of the U.S. Catholic Bishops in Dallas, editors and reporters at The Dallas Morning News wanted to present readers with a different perspective on the issue.

"We were missing the sense that this wasn't unusual," said Brooks Egerton, a reporter for The Morning News. "As much as everyone was reporting on the trees, no one was covering the forest."

Building on work he had done for The Morning News in 1997 covering one of the largest clergy abuse cases to go to trial, Egerton said he had come to understand that what was going on with the sex-abuser priests was part of a larger pattern. He dug up some old notes he had from the mid-1990s when the Diocese of Dallas had its own priest sex abuse crisis and followed up on trends he had observed at the time.

The Morning News created a database in Microsoft Access to track how church management-mainly bishops-had allowed priests accused of sexual abuse to keep working. Egerton said this was the central issue of the recent scandal in the Catholic Church.

The original purpose of the database was an attempt to analyze the trends Egerton had observed. "We started off simply trying to quantify how many U.S. diocesan leaders had done what Cardinal Law had done - keep accused priests on the job," Egerton said. As the database grew in size and depth, The Morning News began questioning how to best present the findings of the database to readers.

The Morning News published information from the database in print and posted it online. The printed version covered five full newspaper pages, Egerton said. A searchable version of the database is available online at www.dallasnews.com/cgi-bin/2002/priests.cgi and allows users to track bishops in their state or diocese, and learn details about their actions.

National scope
The New York Times wanted to conduct a study that would measure the scope of sexual abuse by priests at a national level. The dilemma The Times' reporters encountered was how to establish facts and detect broad trends about a subject shrouded in secrecy without existing data.

"The limitations of this kind of project are getting to the difficulties of systematically counting details of sexual abuse," said Josh Barbanel, a reporter at The Times specializing in computer-assisted reporting. The Times had to devise a way to gather and analyze information documenting priests accused of sexual abuse.

Database editor Tom Torok created a

20
May • June 2003

==End of OCR for page 20==

==Start of OCR for page 21==
Web data-entry form using Active Server Pages (ASP) that connected to a SQL Server database. Research staff and clerks typed information compiled from a variety of sources, including newspaper articles, court records and church records. Some of the information recorded in the database included the state or location of the report, parish information information about the priests.

After completion of the data-entry process, Barbanel cleaned the database in Microsoft Access. He then analyzed the data searching for trends. Through this data work, The Times was able to identify 1,205 priests accused of sexually abusing minors. The data analysis showed a spike in the number of priests accused of abuse who were ordained in the 1970s. The report also identified victims based on age and gender and listed the number of victims per abuser-priest.

According to "How the study of Sexual Abuse by Priests Was Conducted," written by Barbanel and published in The Times Jan. 12, the analysis found "1.8 percent of priests ordained since 1950 have been accused of sexually abusing minors, including nearly 3.3 percent of priests ordained in two particular years, 1970 and 1975."

The article detailed the methods and shortcomings of the analysis: "The study, although the most complete of its kind, faces some methodological limitations that make it difficult for either supporters or opponents of the church to draw sweeping conclusions." Barbanel said that for this kind of study is it important to explain the methods, definitions and how the data worked into the rest of the story.

Some of the limits of this type of an analysis are simply due to the limits of the information. Torok said this is common and that journalists should approach numbers cautiously regardless of the project.

"Sometimes purportedly accurate numbers are horrendous," Torok said. "You have to let people know the limitations, the downside of things and use words like 'about'." To handle some of the limits with this particular project, Barbanel said, The Times tried to carefully define the universe of data used. In doing this, the study included only ordained priests and excluded Catholic lay ministers.

While the Roman Catholic Church and the public continue to grapple with sexual abuse by priests, journalists are continuing to uncover even more evidence of wrongdoing. The Globe posts continuing and archived coverage on its Web site at www.boston.com/globe/spotlight/abuse. Other news organizations are also attempting to track evidence of abuse within the church. On May 4, for example, The Arizona Republic published "Priests with troubled pasts," which documents problems within the Roman Catholic Diocese of Phoenix.

According to the story, The Republic analyzed 33 years of church directories and checked the names of priests in public records. According to the newspaper, the data analysis found "Priests accused of sexual misconduct at some point during their careers were assigned to at least 42 of the 88 parishes in the Phoenix Diocese from 1970 to 2002."

Contact Jaimi Dowdell by e-mail at jaimi@nicar.org.

Uplink

readme.txt
The IRE Resource Center has copies of the stories mentioned in this article.

Story No. 19857: "Crisis in the Catholic Church." The Boston Globe series tracks decades of abuse within the Roman Catholic Church and the efforts of church leaders to cover up the problem.

Story No. 19855: "Catholic Bishops and Sex Abuse." The Dallas Morning News exposes the negligence of U.S. bishops in failing to stop sexual misconduct within the Catholic Church, even after church leaders agreed on a policy to suspend errant priests.

Story No. 20219: "Decades of Damage." A New York Times survey finds more than 1,200 priests were involved in the abuse of more than 4,000 minors in the last 60 years.

The Arizona Republic story. "Priests with troubled pasts," will soon be available from the IRE Resource Center. Search the story database at www.ire.org/resourcecenter.

To order a story, call the Resource Center at 573-882-3364 or rescntr@nicar.org.

REQUIRED READING
For Your Newsroom
Numbers In The Newsroom
Using Math and Statistics in News, by Sarah Cohen

Pulitzer Prize-winning journalist Sarah Cohen guides reporters through fractions, rates, percents and per capitas. Making inflation adjustments. Understanding averages. Doing the budget story. Questioning surveys and polls.
[Image of the book cover for "Numbers in the Newsroom"]

ORDER TODAY!
Call 573-882-3364 with your VISA or MasterCard
-OR-
Visit our Web site at www.ire.org/store for online ordering or order form downloads
IRE MEMBERS: $15 each • NONMEMBERS: $25 each

May • June 2003
21

==End of OCR for page 21==

==Start of OCR for page 22==
fait our Weis www.nicar.org

Contracts
continued from page 1

tell readers what work their neighbors performed for the government rather than focus on a local company handling a contract thousands of miles away. We still wanted to know the totals for the North and South Carolina-based contractors; that just wasn't the main part of the story.

We filtered for North Carolina and South Carolina using the "stateperf" - or state performed - field. Once we did that we ran a variety of sorts, sums and group bys, including by city, county, business and industry. But with nearly 11,000 contracts we still had to find diverse examples of services or risk overlooking something good. That's where it helped to sum and group by the North American Industry Classification System (NAICS) description. That turned up a funeral home and the federal prison system. We also grouped by the product description and found contracts for educational toys at military schools.

The product descriptions, however, weren't terribly helpful. And sometimes they were unintentionally deceptive. One listed a company with the promising product "hardware, weapon system." But, when I called the company, a representative told me they repair aircraft, and that the military considers every such plane a "weapon.”

So we learned a lesson: When citing a company in the story be sure to talk to them and ask precisely what they make. Also double check where the contract is handled. Another local company had an equally interesting description of "guided missile remote control system". But when we called them, they said the division's headquarters was in North Carolina, but the work was actually performed in Texas.

Like any other database, the Federal Procurement Data System contains dirty data. In one case the database listed more than 50 contracts for a furniture company. When I summed by company name and sorted, this company topped the list. Its contracts ranged from $27,000 to $688,000. But one contract was listed at $99.9 million, about $99.8 million more than the contract was really worth.

Keep it fresh
Because the NICAR data was from the 2001 fiscal year, we made a point of asking each company if they still had contracts with the military to give the story a timelier feel.

The database lists the total dollars obligated by the contract for that particular fiscal year. So depending on the structure of the contract or when the money was due, the recipient may not have gotten every cent that particular year. Most recipients were willing to detail the money from their contracts.

Sometimes the amount field for a contract contains a negative number. That's not error; it reflects normal contract adjustments during that fiscal year. (For more details about working with amounts in the data, see "Getting a handle on shuttle contractors" in the March/April Uplink).

After interviewing the data, we interviewed people. It's good to ask to speak to reservists or vets at the companies in addition to the company officials. We found one man who helped run a local lithium battery plant and then had gone to Afghanistan where the night-vision goggles he used were powered by batteries made at the plant.

Once we got the data, the story took less than three weeks to do.

Despite the heavy database work at the front end, the story wound up as more of a feature about how Carolina companies were gearing up for the war and an overview of the scope of military work performed here. We also ran a map of the Carolinas showing the value of total work performed in each county based on data that we analyzed in ArcView 8 geographic information system (GIS) software.

There are other fields in the database that could produce good stories about minority-, veteran- or women-owned companies. Journalists could also use the data to find nonprofit groups reaping money from military contracts and inmates performing contracts for the military.

Other data
Shortly before deadline we came across a U.S. Department of Defense Web site with links to procurement and other data (www.dior.whs.mil/DIORHOME.htm). The procurement files are provided as exclamation point delimited text.

From that Web page we were able to get 2002 fiscal year totals by clicking on procurement, then statistics, then ST25. You can also download entire data for individual fiscal years back to 1966 at www.dior.whs.mil/peidhome/guide/procoper.htm.

The San Francisco Chronicle analyzed this data for its March 23 report about the billions of dollars flowing yearly from the U.S. military to companies in the Bay area.

Downloading and unzipping the data takes a long time. Pay attention to the record layout because it sometimes changes from year to year.

There are a number of differences between the DOD and Federal Procurement data, primarily that the DOD includes all contracts, not just those worth more than $25,000.

The Federal Procurement data provided by NICAR has field headers and additional fields, such as descriptions of the NAICS codes.

If you're just getting into databases and work with Access, you might be more comfortable with the easier-to-use NICAR database.

Contact Adam Bell by e-mail at abbell@charlotteobserver.com.

22
May • June 2003

==End of OCR for page 22==

==Start of OCR for page 23==
Awards
continued from page 3

lapse of Enron and its extensive record of campaign donations. The same 24 executives who made large donations to the Bush campaign also unloaded $1.1 billion in stock before the company went under. Journalists analyzed Federal Elections Commission data with Microsoft Excel to make these connections.(Story No. 19452)

Special Categories
Student Work - Finalists
Capital News Service (University of Maryland): "Killer Coasters" by Kathleen Johnston Jarboe. Jarboe investigated rates of injuries on roller coasters that had been deemed safe by ride inspectors. She used Access and Excel to examine a state database of safety inspections of amusement park rides. (Story No. 19442)

Capital News Service (University of Maryland): "State Salaries" by Hanah Cho. Cho found that top university officials earned many times the salary of the state's governor and other state officials. She used Access and Excel to analyze spreadsheets of salary information for the highest salaries earned in Maryland. (Story No. 19441)

Capital News Service (University of Maryland): "Foster Shuffle" by Michelle Krupa. This story investigated cases in which children put into foster care because their parents were accused of sexual abuse wound up back in their homes. Krupa used Access and Excel to examine the federal Adoption and Foster Care Analysis and Reporting System database kept by the National Data Archive on Child Abuse and Neglect. (Story No. 19440)

readme.txt
Contact the IRE Resource Center at 573-882-3364 or rescntr@ire.org to order any of these stories.

Uplink
IRE and NICAR Services

Investigative Reporters and Editors, Inc. is a grassroots nonprofit organization dedicated to improving the quality of investigative reporting within the field of journalism. IRE was formed in 1975 with the intent of creating a networking tool and a forum in which journalists from across the country could raise questions and exchange ideas. IRE provides educational services to reporters, editors and others interested in investigative reporting and works to maintain high professional standards.

Programs and Services
IRE Resource Center: A rich reserve of print and broadcast stories, tipsheets and guides to help you start and complete the best work of your career. This unique library is the starting point of any piece you're working on. You can search through abstracts of more than 19,000 investigative reporting stories through our Web site.
Contact: Carolyn Edds,
carolyn@ire.org, 573-882-3364

Database Library: Administered by IRE and the National Institute for Computer-Assisted Reporting. The library has copies of many government databases, and makes them available to news organizations at or below actual cost. Analysis services are available on these databases, as is help in deciphering records you obtain yourself.
Contact: Jeff Porter,
jeff@ire.org, 573-882-1982

Campaign Finance Information Center: Administered by IRE and the National Institute for Computer-Assisted Reporting. It's dedicated to helping journalists uncover the campaign money trail. State campaign finance data is collected from across the nation, cleaned and made available to journalists. A search engine allows reporters to track political cash flow across several states in federal and state races.
Contact: Brant Houston,
brant@ire.org, 573-882-2042

On-the-Road Training: As a top promoter of journalism education, IRE offers loads of training opportunities throughout the year. Possibilities range from national conferences and regional workshops to weeklong boot camps and on-site newsroom training. Costs are on a sliding scale and fellowships are available to many of the events.
Contact: Ron Nixon,
ron@nicar.org, 573-882-2042

Publications
The IRE Journal: Published six times a year. Contains journalist profiles, how-to stories, reviews, investigative ideas and backgrounding tips. The Journal also provides members with the latest news on upcoming events and training opportunities from IRE and NICAR.
Contact: Len Bruzzese,
len@ire.org, 573-882-2042

Uplink: Bimonthly newsletter by IRE and NICAR on computer-assisted reporting. Often, Uplink stories are written after reporters have had particular success using data to investigate stories. The columns include valuable information on advanced database techniques as well as success stories written by newly trained CAR reporters.
Contact: David Herzog,
dherzog@nicar.org, 573-882-2127

Reporter.org: A collection of Web-based resources for journalists, journalism educators and others. Discounted Web hosting and services such as mailing list management and site development are provided to other nonprofit journalism organizations.
Contact: Ted Peterson,
ted@nicar.org, 573-884-7321

For information on:
Advertising: Pia Christensen,
pia@ire.org, 573-884-2175

Membership and subscriptions:
John Green,
jgreen@ire.org, 573-882-2772

Conferences and Boot Camps:
Ev Ruch-Graham,
ev@ire.org, 573-882-8969

Listservs: Ted Peterson,
ted@nicar.org, 573-884-7321

Mailing Address:
IRE, 138 Neff Annex, Missouri School of Journalism, Columbia, MO 65211

May • June 2003
23

==End of OCR for page 23==

==Start of OCR for page 24==
Investigative Reporters and Editors, Inc.
138 Neff Annex
Missouri School of Journalism
Columbia, MO 65211

NON-PROFIT ORG.
U.S. POSTAGE
PAID
Jefferson City, MO.
PERMIT NO. 89

Ira Chinoy
Philip Merrill College of Journalism
1117 JOURNALISM BLDG
UNIVERSITY OF MARYLAND
COLLEGE PARK MD 20742-0001

Uplink Info
A newsletter of the National Institute for Computer-Assisted Reporting

Editor
Brant Houston
brant@ire.org

Managing Editor
David Herzog
dherzog@nicar.org

Asst. Managing Editor
Jeff Porter
jeff@nicar.org

Art Director
Lisa Triefenbach

Copy Editor
Pia Christensen

Contributing Editors
Jaimi Dowdell
Holly Hacker
Stephanie Kang

NICAR is a joint program
of Investigative Reporters and
Editors Inc. and the Missouri
School of Journalism.

NICAR services include
supplying journalists with
government databases,
training programs, tipsheets and
data analysis.

Editorial
573-884-7711

Subscriptions
573-882-2772

Director of Publications
Len Bruzzese
len@ire.org

Advertising Coordinator
Pia Christensen
pia@ire.org

Subscription Administrator
John Green
jgreen@ire.org

Subscriptions
IRE members $40, nonmembers $60

Uplink Address:
IRE-NICAR, 138 Neff Annex
Missouri School of Journalism
Columbia, MO 65211

Postmaster: Please send
address changes to IRE-NICAR.

Uplink

==End of OCR for page 24==
